{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.4\n"
     ]
    }
   ],
   "source": [
    "import scipy as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /mnt/DATA/projects/signals/venv/lib/python3.7/site-packages (1.5.4)\r\n",
      "Requirement already satisfied: numpy>=1.14.5 in /mnt/DATA/projects/signals/venv/lib/python3.7/site-packages (from scipy) (1.19.4)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall -y tensorflow-gpu tensorflow keras\n",
    "#!pip install --upgrade pip\n",
    "#!pip install  --upgrade --force-reinstall keras tensorflow-gpu\n",
    "#!pip install verta\n",
    "#!pip install numpy\n",
    "#!pip install sklearn\n",
    "#!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os,random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pickle as cPickle, random, sys\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from numpy.fft import *\n",
    "\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successfully established\n"
     ]
    }
   ],
   "source": [
    "#import verta\n",
    "from verta import Client\n",
    "client = Client('http://138.75.233.102:3000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Project: Deepsig IQ data Detection in personal workspace\n",
      "created new Experiment: Time series Classification\n"
     ]
    }
   ],
   "source": [
    "proj = client.set_project(\"Deepsig IQ data Detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd = cPickle.load(open(\"RML2016.10b.dat\",'rb'), encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENSITY_AXIS=0\n",
    "QUADRATURE_AXIS=1\n",
    "\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Intensity = []  \n",
    "X_Quadrature = []\n",
    "Label_strings = []\n",
    "for mod in mods:\n",
    "    X_Intensity.append(Xd[(mod,18)][:,INTENSITY_AXIS,:])\n",
    "    X_Quadrature.append(Xd[(mod,18)][:,QUADRATURE_AXIS,:])\n",
    "    for label in range(len(Xd[(mod,18)][:,INTENSITY_AXIS,:])):\n",
    "        Label_strings.append(mod)\n",
    "        \n",
    "X_Intensity = np.vstack(X_Intensity) # Quick way to create stacked dataset\n",
    "X_Quadrature = np.vstack(X_Quadrature)\n",
    "Label_strings=np.vstack(Label_strings)\n",
    "complex_num = X_Intensity + 1j*X_Quadrature\n",
    "\n",
    "le= preprocessing.LabelEncoder()\n",
    "le.fit(Label_strings)\n",
    "Label= le.transform(Label_strings)\n",
    "depth = 10\n",
    "Label = tf.one_hot(Label,depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['8PSK'],\n",
       "       ['8PSK'],\n",
       "       ['8PSK'],\n",
       "       ...,\n",
       "       ['WBFM'],\n",
       "       ['WBFM'],\n",
       "       ['WBFM']], dtype='<U6')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8PSK', 'AM-DSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']\n"
     ]
    }
   ],
   "source": [
    "print((mods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 138)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2016)\n",
    "X_Intensity_Dataset = np.c_[X_Intensity, Label]\n",
    "X_Intensity_Dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_Intensity_Dataset_Label_shuffled = copy.copy(X_Intensity_Dataset)\n",
    "np.random.shuffle(X_Intensity_Dataset_Label_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000\n"
     ]
    }
   ],
   "source": [
    "length_of_X_train = int(0.7*X_Intensity_Dataset_Label_shuffled.shape[0]) #42000\n",
    "length_of_X_validation = (int(0.2*X_Intensity_Dataset_Label_shuffled.shape[0]))+length_of_X_train #12000\n",
    "print(length_of_X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 128)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Intensity_Dataset = X_Intensity_Dataset_Label_shuffled[0:length_of_X_train,0:128]\n",
    "Y_Train_Intensity_Dataset = X_Intensity_Dataset_Label_shuffled[0:length_of_X_train,128:]\n",
    "\n",
    "Validation_Intensity_Dataset = X_Intensity_Dataset_Label_shuffled[length_of_X_train:length_of_X_validation,0:128]\n",
    "Y_Validation_Intensity_Dataset = X_Intensity_Dataset_Label_shuffled[length_of_X_train:length_of_X_validation,128:]\n",
    "\n",
    "Validation_Intensity_Dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Y_Train_Intensity_Dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got existing Experiment: Convolution 1D \n",
      "created new ExperimentRun: Convolution 1D_drop0.1_5Layers\n"
     ]
    }
   ],
   "source": [
    "# Y_train.shape\n",
    "BATCH_SIZE = 100\n",
    "expt = client.set_experiment(\"Convolution 1D \")\n",
    "run_2 = client.set_experiment_run(\"Convolution 1D_drop0.1_5Layers\")\n",
    "run_2.log_hyperparameter(\"num_layers\", 5)\n",
    "run_2.log_hyperparameter(\"dropout\", 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_35 (Conv1D)           (None, 119, 100)          1100      \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 110, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 36, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 27, 160)           160160    \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 18, 160)           256160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1610      \n",
      "=================================================================\n",
      "Total params: 519,130\n",
      "Trainable params: 519,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D,Conv1D,MaxPooling1D\n",
    "model_m = Sequential()\n",
    "model_m.add(Conv1D(100, 10, activation='relu', input_shape=(128,1)))\n",
    "model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "model_m.add(MaxPooling1D(3))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(GlobalMaxPooling1D())\n",
    "model_m.add(Dropout(0.1))\n",
    "model_m.add(Dense(10, activation='softmax'))\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Train_Intensity_Dataset=Train_Intensity_Dataset.reshape(42000,128,1)\n",
    "Validation_Intensity_Dataset=Validation_Intensity_Dataset.reshape(12000,128,1)\n",
    "print(Validation_Intensity_Dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "  1/420 [..............................] - ETA: 0s - loss: 2.3025 - accuracy: 0.0900WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0069s). Check your callbacks.\n",
      "420/420 [==============================] - ETA: 0s - loss: 1.5867 - accuracy: 0.3690WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_test_batch_end` time: 0.0015s). Check your callbacks.\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 1.5867 - accuracy: 0.3690 - val_loss: 1.2655 - val_accuracy: 0.4820\n",
      "Epoch 2/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 1.0715 - accuracy: 0.5386 - val_loss: 0.9072 - val_accuracy: 0.5979\n",
      "Epoch 3/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.8902 - accuracy: 0.6073 - val_loss: 0.8305 - val_accuracy: 0.6323\n",
      "Epoch 4/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.8228 - accuracy: 0.6342 - val_loss: 0.7887 - val_accuracy: 0.6368\n",
      "Epoch 5/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.7843 - accuracy: 0.6508 - val_loss: 0.7511 - val_accuracy: 0.6572\n",
      "Epoch 6/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.7504 - accuracy: 0.6639 - val_loss: 0.7374 - val_accuracy: 0.6624\n",
      "Epoch 7/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.7323 - accuracy: 0.6712 - val_loss: 0.7132 - val_accuracy: 0.6722\n",
      "Epoch 8/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.7113 - accuracy: 0.6806 - val_loss: 0.7444 - val_accuracy: 0.6637\n",
      "Epoch 9/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.7010 - accuracy: 0.6872 - val_loss: 0.7053 - val_accuracy: 0.6693\n",
      "Epoch 10/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.6783 - accuracy: 0.6953 - val_loss: 0.6928 - val_accuracy: 0.6841\n",
      "Epoch 11/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.6660 - accuracy: 0.7028 - val_loss: 0.6999 - val_accuracy: 0.6777\n",
      "Epoch 12/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.6545 - accuracy: 0.7043 - val_loss: 0.6673 - val_accuracy: 0.6891\n",
      "Epoch 13/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.6407 - accuracy: 0.7123 - val_loss: 0.6527 - val_accuracy: 0.6975\n",
      "Epoch 14/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.6356 - accuracy: 0.7128 - val_loss: 0.6587 - val_accuracy: 0.6958\n",
      "Epoch 15/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.6249 - accuracy: 0.7142 - val_loss: 0.6448 - val_accuracy: 0.7078\n",
      "Epoch 16/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.6038 - accuracy: 0.7272 - val_loss: 0.6385 - val_accuracy: 0.7097\n",
      "Epoch 17/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5951 - accuracy: 0.7294 - val_loss: 0.6496 - val_accuracy: 0.7023\n",
      "Epoch 18/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5941 - accuracy: 0.7300 - val_loss: 0.6610 - val_accuracy: 0.6980\n",
      "Epoch 19/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5810 - accuracy: 0.7351 - val_loss: 0.6393 - val_accuracy: 0.7048\n",
      "Epoch 20/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5725 - accuracy: 0.7392 - val_loss: 0.6311 - val_accuracy: 0.7123\n",
      "Epoch 21/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5655 - accuracy: 0.7419 - val_loss: 0.6382 - val_accuracy: 0.7083\n",
      "Epoch 22/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5550 - accuracy: 0.7446 - val_loss: 0.6249 - val_accuracy: 0.7184\n",
      "Epoch 23/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5460 - accuracy: 0.7489 - val_loss: 0.6413 - val_accuracy: 0.7172\n",
      "Epoch 24/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5351 - accuracy: 0.7552 - val_loss: 0.6388 - val_accuracy: 0.7128\n",
      "Epoch 25/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5296 - accuracy: 0.7579 - val_loss: 0.6183 - val_accuracy: 0.7203\n",
      "Epoch 26/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5201 - accuracy: 0.7595 - val_loss: 0.6152 - val_accuracy: 0.7165\n",
      "Epoch 27/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5147 - accuracy: 0.7643 - val_loss: 0.6291 - val_accuracy: 0.7198\n",
      "Epoch 28/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.5045 - accuracy: 0.7654 - val_loss: 0.6442 - val_accuracy: 0.7092\n",
      "Epoch 29/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.4998 - accuracy: 0.7716 - val_loss: 0.6365 - val_accuracy: 0.7095\n",
      "Epoch 30/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.4903 - accuracy: 0.7747 - val_loss: 0.6125 - val_accuracy: 0.7215\n",
      "Epoch 31/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.4912 - accuracy: 0.7745 - val_loss: 0.7197 - val_accuracy: 0.6933\n",
      "Epoch 32/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.4816 - accuracy: 0.7777 - val_loss: 0.6368 - val_accuracy: 0.7208\n",
      "Epoch 33/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.4806 - accuracy: 0.7804 - val_loss: 0.6271 - val_accuracy: 0.7215\n",
      "Epoch 34/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.4650 - accuracy: 0.7872 - val_loss: 0.6351 - val_accuracy: 0.7233\n",
      "Epoch 35/300\n",
      "420/420 [==============================] - 3s 6ms/step - loss: 0.4594 - accuracy: 0.7883 - val_loss: 0.6491 - val_accuracy: 0.7209\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "history = model_m.fit(Train_Intensity_Dataset,\n",
    "                      Y_Train_Intensity_Dataset,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                     validation_data=(Validation_Intensity_Dataset, Y_Validation_Intensity_Dataset),\n",
    "                    callbacks = [tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6491 - accuracy: 0.7209\n",
      "Accuracy: 72.09\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model_m.evaluate(Validation_Intensity_Dataset, Y_Validation_Intensity_Dataset)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "\n",
    "run_2.log_metric(\"accuracy\", accuracy)\n",
    "run_2.log_tags([\"Experiment_dropout0.1_5Layers\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0X0lEQVR4nO3deXxU1f3/8ddnZrJNErKzJmFTFgHZgqCoBbcColhFK1oVl1Krbe2iVfu1tdX6+9pqW+u3LnVvrRW3iiiiVlrEFhFB2fc9Ycu+kXVmzu+PMwkBspFMMpnJ5/l4zGMy996599wQ3nPm3HPOFWMMSimlQp8j2AVQSikVGBroSikVJjTQlVIqTGigK6VUmNBAV0qpMKGBrpRSYUIDXXUoEVksIjcEettgE5HvishhESkXkZRgl0cpANF+6Op4IlLe4KUbqAa8/tffMca80vml6jpEJAIoBSYZY9YGuzxK1dFAV80SkT3ALcaYjxtZ5zLGeDq/VMEjIi6gN5ANRJzs+YuIYP/f+TqifKp70yYX1WoiMkVEckTkbhE5BLwoIkki8p6I5IlIkf/n9AbvWSoit/h/nisi/xGRR/3b7haR6W3cdqCILBORMhH5WESeEJG/tVDun4lIvojsEZFrG6yP8h9nn78Z5WkRiWninF8GtvrfWiwi//Jvd5aIfCEiJf7ns447r4dE5L9ABTBIRIyI3CYi2/3n8KCIDBaR5SJSKiKvi0ik//2t+R0/KCL/9e/rIxFJbbD+bP9+i0UkW0TmtnTeKjRpoKuT1RtIBvoD87B/Qy/6X2cClcCfmnn/RGwgpgK/BZ7311pPdtu/AyuBFOCXwHWtKHcq0A+4AXhGRIb61z0MDAHGAKf4t/lFE+d8EzDCvzzRGHOeiCQDi4DH/eX5PbDouLb167C/r3hgr3/Z14HxwCTgp8AzwLeADGAkMMe/XWt+x9cANwI9gUjgTgAR6Q8sBv4PSPOf45pWnrcKNcYYfeijyQewB7jA//MUoAaIbmb7MUBRg9dLsU02AHOBHQ3WuQED9D6ZbbGh5gHcDdb/DfhbE2Wa4t8+tsGy14GfAwIcAQY3WHcmsLupcwYG+Mvi8r++Dlh53DE/A+Y2OK8HjltvgMkNXq8G7m7w+nfAYyfxO76vwevbgA/8P98LvN3IPpo9b32E5sPVRM4r1ZQ8Y0xV3QsRcQN/AKYBSf7F8SLiNMZ4G3n/obofjDEV/gp3XBPHamrbVKDQGFPRYNtsbM22KUXGmCMNXu8F+mJrrW5gdYMvCgI4G2x7zDk3oi9Ha90N99/vuPId73CDnysbed0bWv07PtTgvRUc/Z1mADsbOXZrzluFGG1yUSfr+KvoPwGGAhONMT2Ac/3Lm2pGCYSDQLI/6Oo0F+YASSIS2+B1JnAAyMeG5whjTKL/kWCMafgh01LPgQPY5pCGMoH9J7GP5rTnd5wNDG5keWvOW4UYDXTVXvHYYCj2tyXf39EHNMbsBVYBvxSRSBE5E7ikFW/9lX/7c4CZwBvG9jZ5FviDiPQEEJF+IvL1kyjS+8AQEblGRFwi8k3gNOC9kzmvZrTnd/wKcIGIXOUvW4qIjAnQeasuRgNdtddjQAy2xrcC+KCTjnstts23APg18Bq2v3xTDgFF2Nr0K8Ctxpgt/nV3AzuAFSJSCnyMrRG3ijGmAPsB8RN/eX4KzDTG5J/MCTXjMdr4OzbG7ANm+MtWiL0gOtq/ul3nrboe7YeuwoKIvAZsMcacUHsVkSnYC6bpx69TKpxoDV2FJBGZ4O+37RCRacAsYEGQi6VUUGkvFxWqegP/wPb7zgG+a4z5KrhFUiq4tMlFKaXChDa5KKVUmAhak0tqaqoZMGBAsA6vlFIhafXq1fnGmLTG1gUt0AcMGMCqVauCdXillApJInL8qOR62uSilFJhQgNdKaXChAa6UkqFCe2HrpRqt9raWnJycqiqam5SSnUyoqOjSU9PJyIiotXv0UBXSrVbTk4O8fHxDBgwgKbvV6JayxhDQUEBOTk5DBw4sNXv0yYXpVS7VVVVkZKSomEeICJCSkrKSX/j0UBXSgWEhnlgteX3GXKBvvVQGY9+uJWiIzXBLopSSnUpLQa6iLwgIrkisqGJ9Qki8q6IrBWRjSJyY+CLedTu/CP86d87OFBS2ZGHUUqpkNOaGvpL2HsZNuV2YJMxZjT2hrq/E5HI9hetcYlue8W3pKK2ow6hlApBxcXFPPnkkyf9vhkzZlBcXHzS75s7dy5vvvnmSb+vI7UY6MaYZdg7nTS5CfaGtYK9MW0h9g7rHaIu0IsrNdCVUkc1FegeT/Nx9P7775OYmNhBpepcgei2+CdgIfbWXvHAN/33KzyBiMwD5gFkZma26WCJMbbyX6w1dKW6pF+9u5FNB0oDus/T+vbg/ktGNLvNPffcw86dOxkzZgwRERFER0eTlJTEli1b2LZtG5dddhnZ2dlUVVVxxx13MG/ePODovFLl5eVMnz6ds88+m+XLl9OvXz/eeecdYmJiWizfkiVLuPPOO/F4PEyYMIGnnnqKqKgo7rnnHhYuXIjL5eKiiy7i0Ucf5Y033uBXv/oVTqeThIQEli1bFpDfEQTmoujXsfcp7AuMAf4kIj0a29AY84wxJssYk5WW1uhkYS06WkPXi6JKqaMefvhhBg8ezJo1a3jkkUf48ssv+eMf/8i2bdsAeOGFF1i9ejWrVq3i8ccfp6Cg4IR9bN++ndtvv52NGzeSmJjIW2+91eJxq6qqmDt3Lq+99hrr16/H4/Hw1FNPUVBQwNtvv83GjRtZt24d9913HwAPPPAAH374IWvXrmXhwoUB/R0EooZ+I/CwsXfK2CEiu4FhwMoA7PsE0RFOolwObUNXqotqqSbdWc4444xjBuU8/vjjvP322wBkZ2ezfft2UlJSjnnPwIEDGTNmDADjx49nz549LR5n69atDBw4kCFDhgBwww038MQTT/C9732P6Ohobr75ZmbOnMnMmTMBmDx5MnPnzuWqq67i8ssvD8CZHhWIGvo+4HwAEemFvWv4rgDst0mJ7ghtclFKNSs2Nrb+56VLl/Lxxx/z2WefsXbtWsaOHdvooJ2oqKj6n51OZ4vt781xuVysXLmS2bNn89577zFtmu1b8vTTT/PrX/+a7Oxsxo8f3+g3hTYfs6UNRORVbO+VVBHJAe4HIgCMMU8DDwIvich6QIC7jTH5ASthIxJjIimq0CYXpdRR8fHxlJWVNbqupKSEpKQk3G43W7ZsYcWKFQE77tChQ9mzZw87duzglFNO4eWXX+ZrX/sa5eXlVFRUMGPGDCZPnsygQYMA2LlzJxMnTmTixIksXryY7OzsE74ptFWLgW6MmdPC+gPARQEpTSsluiO0l4tS6hgpKSlMnjyZkSNHEhMTQ69everXTZs2jaeffprhw4czdOhQJk2aFLDjRkdH8+KLL3LllVfWXxS99dZbKSwsZNasWVRVVWGM4fe//z0Ad911F9u3b8cYw/nnn8/o0aMDVpag3SQ6KyvLtPWORd95eRV78iv48EfnBrhUSqm22Lx5M8OHDw92McJOY79XEVltjMlqbPuQG/oPtslFe7kopdSxQnL6XL0oqpTqLLfffjv//e9/j1l2xx13cOONHTrLSZuEZKAnuCOo9vioqvUSHeEMdnGUUmHsiSeeCHYRWi1km1xAR4sqpVRDoRnoOlpUKaVOEJqBHuMPdK2hK6VUvdAMdHddk4vW0JVSMHXqVD788MNjlj322GN897vfbXT7KVOmUNdtuqnpc3/5y1/y6KOPNnvcBQsWsGnTpvrXv/jFL/j4449PsvSBE6KBrjV0pdRRc+bMYf78+ccsmz9/PnPmNDsuEmjf9LnHB/oDDzzABRdc0KZ9BUJoB7qOFlVKAbNnz2bRokXU1Nhv7Xv27OHAgQO8+uqrZGVlMWLECO6///5G3ztgwADy8+1sJQ899BBDhgzh7LPPZuvWrfXbPPvss0yYMIHRo0dzxRVXUFFRwfLly1m4cCF33XUXY8aMYefOncfc9GLJkiWMHTuWUaNGcdNNN1FdXV1/vPvvv59x48YxatQotmzZErDfQ0h2W4yJcBLpdGgNXamuaPE9cGh9YPfZexRMf7jJ1cnJyZxxxhksXryYWbNmMX/+fK666ip+9rOfkZycjNfr5fzzz2fdunWcfvrpje5j9erVzJ8/nzVr1uDxeBg3bhzjx48H4PLLL+fb3/42APfddx/PP/883//+97n00kuZOXMms2fPPmZfdVPqLlmyhCFDhnD99dfz1FNP8cMf/hCA1NRUvvzyS5588kkeffRRnnvuuQD8kkK0hi4iJLgjKNFeLkopv4bNLnXNLa+//jrjxo1j7NixbNy48ZjmkeN9+umnfOMb38DtdtOjRw8uvfTS+nUbNmzgnHPOYdSoUbzyyits3Lix2bI0NqVuwxtZ1E2b29opelsrJGvoYHu6aA1dqS6omZp0R5o1axY/+tGP+PLLL6moqCA5OZlHH32UL774gqSkJObOndvolLmtMXfuXBYsWMDo0aN56aWXWLp0abvKWjdNb3un6D1eSNbQQYf/K6WOFRcXx9SpU7npppuYM2cOpaWlxMbGkpCQwOHDh1m8eHGz7z/33HNZsGABlZWVlJWV8e6779avKysro0+fPtTW1vLKK6/UL29qyt6GU+oC9VPqdrSQDfSEmEi9KKqUOsacOXNYu3Ytc+bMYfTo0YwdO5Zhw4ZxzTXXMHny5GbfO27cOL75zW8yevRopk+fzoQJE+rXPfjgg0ycOJHJkyczbNiw+uVXX301jzzyCGPHjmXnzp31yxtOqTtq1CgcDge33npr4E/4OCE5fS7AnW+sZfmOfJbfe34AS6WUagudPrdjdIvpcwGS3BEUaZOLUkrVC9lAT3RHUlnrparWG+yiKKVUlxCygZ7gn8+lVNvRleoSgtV8G67a8vsM2UDX0aJKdR3R0dEUFBRoqAeIMYaCggKio6NP6n0h3A9d50RXqqtIT08nJyeHvLy8YBclbERHR5Oenn5S7wndQK+foEtHiyoVbBEREQwcODDYxej2QrbJpa4NXZtclFLKCtlAr6uhl2iTi1JKASEc6HFRLlwO0dvQKaWUX4uBLiIviEiuiGxoZpspIrJGRDaKyCeBLWKTxyRRBxcppVS91tTQXwKmNbVSRBKBJ4FLjTEjgCsDUrJWSIiJ0CYXpZTyazHQjTHLgMJmNrkG+IcxZp9/+9wAla1Fie5IbXJRSim/QLShDwGSRGSpiKwWkesDsM9W0TnRlVLqqED0Q3cB44HzgRjgMxFZYYzZdvyGIjIPmAeQmZnZ7gMnuCPYcujEuYiVUqo7CkQNPQf40BhzxBiTDywDRje2oTHmGWNMljEmKy0trd0HToyJpET7oSulFBCYQH8HOFtEXCLiBiYCmwOw3xYluiMor/ZQ6/V1xuGUUqpLa7HJRUReBaYAqSKSA9wPRAAYY542xmwWkQ+AdYAPeM4Y02QXx0CqH1xUWUtqXFRnHFIppbqsFgPdGDOnFds8AjwSkBKdhPrh/xUa6EopFbIjRQGS3HUzLmrXRaWUCulAPzrjol4YVUqp0A70ujnRtaeLUkqFdqAn6JzoSilVL6QDPT7KhUPQvuhKKUWIB7rDISTo8H+llAJCPNChboIuDXSllAr5QLc1dG1DV0qpkA/0RHeEtqErpRRhEOhJ7kiKtIaulFKhH+h6UVQppayQD/REdwRlVR48OuOiUqqbC/1A90/QVVrlCXJJlFIquEI/0HWCLqWUAsIg0OuH/2tPF6VUNxfygV7X5FKiF0aVUt1c6Ad6XZNLpTa5KKW6t9AP9BidE10ppSAMAr1HTAQiUKSBrpTq5kI+0J0OoUd0BCXay0Up1c2FfKCDHVykvVyUUt1deAS6Dv9XSqnwCPQEnRNdKaXCI9ATY7QNXSmlwiPQtQ1dKaXCJNBj7E0ufD4T7KIopVTQtBjoIvKCiOSKyIYWtpsgIh4RmR244rVOgjsSY6C0SmvpSqnuqzU19JeAac1tICJO4DfARwEo00lLcutoUaWUajHQjTHLgMIWNvs+8BaQG4hCnaxEnXFRKaXa34YuIv2AbwBPtWLbeSKySkRW5eXltffQ9RJidE50pZQKxEXRx4C7jTEt3gPOGPOMMSbLGJOVlpYWgENbdTX0Eq2hK6W6MVcA9pEFzBcRgFRghoh4jDELArDvVtEZF5VSKgA1dGPMQGPMAGPMAOBN4LYODfN9K2D+tVB+tMkmQQNdKaVa1W3xVeAzYKiI5IjIzSJyq4jc2vHFa0R1GWx5Dwp31S9yOR3ER7n0JhdKqW6txSYXY8yc1u7MGDO3XaVpjaQB9rl4L2ROrF+c4I7Q29Appbq10BspmpBhn4v2HLM4yR1JkfZyUUp1Y6EX6BHREN/3hEDX+VyUUt1d6AU62GaX4wI9IUabXJRS3VuIBnp/KNp7zCKtoSulursQDfQBULofPNX1ixJjIimuqNEZF5VS3VboBjoGirPrF2Umu/EZ2F1wJGjFUkqpYArNQE/sb5+L99QvGp2RCMCafcWdXhyllOoKQjPQ6/qiN7gwekrPOGIjnazJLg5GiZRSKuhCM9DjeoEr+phAdzqE09MTWZtTHLRiKaVUMIVmoDsckJh5Qk+XMZmJbD5YSlWtN0gFU0qp4AnNQIdG+6KPyUik1mvYeKA0KEVSSqlgCv1AN0e7KY6tuzCq7ehKqW4odAM9sT9Ul0JlUf2inj2i6ZsQrYGulOqWQjfQG8662MDojETWZBeduL1SSoW50A/0RtrRswsrKSivPuEtSikVzkI40P2DixoJdEC7Lyqlup3QDfSoeHCnnNB1cVR6Ak6H6IhRpVS3E7qBDo12XXRHuhjSK56v9MKoUqqbCe1AT+x/QqCDbXZZm12sMy8qpbqV0A70pAFQkg2+Y0eGjslIoLTKozMvKqW6ldAPdJ/Hzo3ewJiMJADWarOLUqobCfFAb7yni868qJTqjkI80AfY5+MCvW7mRQ10pVR3EtqB3iMdxHlC10XQmReVUt1PaAe60wUJ6U32dNGZF5VS3UmLgS4iL4hIrohsaGL9tSKyTkTWi8hyERkd+GI2o5G+6HB0xKg2uyiluovW1NBfAqY1s3438DVjzCjgQeCZAJSr9ZIGnDBBF0CvHtH0SYjWni5KqW6jxUA3xiwDCptZv9wYUze94QogPUBla52k/nAkD6rLT1g1JkMvjCqluo9At6HfDCxuaqWIzBORVSKyKi8vLzBHbGIaXbCBvq+wQmdeVEp1CwELdBGZig30u5vaxhjzjDEmyxiTlZaWFpgD13ddbDzQQWdeVEp1DwEJdBE5HXgOmGWMKQjEPlstcYB9buTC6Kj0BByCzryolOoW2h3oIpIJ/AO4zhizrf1FOknuZIiMbzTQdeZFpVR34mppAxF5FZgCpIpIDnA/EAFgjHka+AWQAjwpIgAeY0xWRxW4kQI22XUR4IyByby+KpvSqlp6REd0WrGUUqqztRjoxpg5Lay/BbglYCVqi6T+ULCj0VVXjEvnr5/t5d21B7h2Yv9OLphSSnWe0B4pWqeuhm5OnP/89PQEhvWO57Uvsju9WEop1ZnCJ9A9VVB++IRVIsLVEzJYl1PCxgMlnV82pZTqJOER6Il10+ie2HUR4LKx/Yh0OXhda+lKqTAWHoHexDS6dRLdkUwf2Zu3v9qvsy8qpcJWeAR6YqZ9biLQAb45IYPSKg8fbDjUOWVSSqlOFh6BHhEN8X0aHf5fZ9LAFPqnuJn/xb5OLJhSSnWe8Ah0aLYvOoDDIVyVlcGKXYXsztebRyulwk+3CXSAK8en43SIdmFUSoWlMAr0gVB6ACqLm9ykZ49ozhvWkzdX51Dr9XVe2ZRSqhOET6APngoY2PZhs5tdPSGD/PJq/rUlt3PKpZRSnSR8Ar1flr0wunlhs5t9bUgavXpEabOLUirshE+gOxww/BLYsQRqmr7o6XI6uHJ8Bku35nKwpLITC6iUUh0rfAIdYPil4KmE7f9sdrOrsjLwGXhjVU4nFUwppTpeeAV65pngTmmx2SUzxc2UoWk8++kuDhRrLV0pFR7CK9CdLhh2sb0wWlvV7Ka/unQEXp/h7rfWYRqZpVEppUJNeAU6wPBZUFMOu5Y2u1n/lFjunTGcT7fn8/eVOnpUKRX6wi/QB54LUQktNrsAfGtiJuecmspDizazr6CiEwqnlFIdJ/wC3RUJQ6fDlkXgrW12UxHhN1ecjlOEu95ci8+nTS9KqdAVfoEOtvtiVTHs+bTFTfsmxvDzS07j892FvLR8T4cXTSmlOkp4Bvop50NELGx+t1WbXzk+nfOH9eQ3H2xhZ155BxdOKaU6RngGekQMnHohbH4PfC3f0EJE+N/LRxEd4eTON9bi1aYXpVQICs9ABzjtUjiSC9mft2rznj2iefCykXy1r5iHF2/WroxKqZATvoF+6kXgjIJNLfd2qXPJ6X24dmImz366m7vfWodHZ2RUSoWQ8A30qHgYfJ5tR29lbVtE+PVlI/n+eafw+qoc5r28mooaTwcXVCmlAiN8Ax1ss0tpDhz4stVvERF+ctFQHrxsJP/emss1z35O4ZGaDiykUkoFRouBLiIviEiuiGxoYr2IyOMiskNE1onIuMAXs42GTAOH66SaXepcN6k/T107nk0HS5n91HKyC3XgkVKqa2tNDf0lYFoz66cDp/of84Cn2l+sAHEn25GjG/4BnpOvZU8b2ZtXbplIfnk1lz+1nPU5JR1QSKWUCowWA90YswwobGaTWcBfjbUCSBSRPoEqYLtN/C6U7IMv/9Kmt08YkMyb3z2LSKeDK55ezhur9MYYSqmuKRBt6P2AhimX4192AhGZJyKrRGRVXl5eAA7dCqdeCP0nwye/bfbGF80Z0iuehd+bTFb/JO56cx33LVhPjUd7wCilupZOvShqjHnGGJNljMlKS0vrnIOKwAW/sn3SVzzZ5t2kxEXx15vO4DvnDuJvK/Zx9TOfcbi0+Sl6lVKqMwUi0PcDGQ1ep/uXdR0ZE2DYTPjv41DRXOtR81xOB/fOGM4T14xjy6EyLn78P6zc3fb9KaVUIAUi0BcC1/t7u0wCSowxBwOw38A67+d2nvRPf9fuXV18eh8W3D6ZHtEu5jy7gnv/sV57wSilgq413RZfBT4DhopIjojcLCK3isit/k3eB3YBO4Bngds6rLTt0XMYjL4GVj4Dxe2/sDmkVzwLvjeZa87I5K3VOUx9dCk/fXMtewva1k6vlFLtJcGasyQrK8usWrWqcw9akgOPj4NRV8JlTwRstwdLKvnzJ7t4deU+PD7DrDF9uX3qKQxOiwvYMZRSCkBEVhtjshpbF94jRY+XkA5nfBvW/h1ytwRst30SYvjlpSP49KdTufGsAby//iAX/WEZDy/eQlVty7M9KqVUIHSvQAc45ycQGQdLHgj4rnv2iOa+mafxn7vP44px/Xj6k51M/+OnrNhVEPBjKaXU8bpfoLuTYfIPYOsi2Ne6qXVPVmpcFL+dPZpXbpmI12e4+hl74bS0qvlb4imlVHt0v0AHmHQbxPWC9++E2soOO8zkU1L58Ifn8u1zBvLaF/u48Pef8MGGQzrXulKqQ3TPQI+MhUseh0PrYNFPWj29blvERDr5n4tP4+3bJpPkjuTWv63myqc/43NthlFKBVj3DHSAodPga/fAmlfgi+c6/HCjMxJ59/tn89A3RpJdVME3n1nB9S+s1Am/lFIB0726LR7P54P5c2DHxzB3EWRO6pTDVtV6+etne3hy6U6KK2qZMao3c88ayMDUWFLjIhGRTimHUir0NNdtsXsHOkBlMTw71U7cNe8T6NF5E0WWVtXy3LJdPPef3VTU2O6N0REO+iXGkJ7kJj0phnNOTeXrI3pryCulAA30lh3eBM9dAL1Hwg3vgSuyUw9feKSGL/cWsb+4kuzCCnKKKskprmBfQQWlVR7GZibyPzOGkzUguVPLpZTqejTQW2PDP+DNGyHrZpj5+2CXBgCvz/Dm6mx+99E2csuq+fqIXtw9bRiDdASqUt2WjhRtjZGXw1k/gFXPw+q23Qwj0JwO4ZsTMll61xR+cuEQ/rM9nwv/sIyfL9jAnnydM0YpdSytoTfk9cDfr4Rdn8DVr8DQ6cEu0THyyqr545JtvLoyG6/PkJns5mtD0jh3SBpnDk4hLsoV7CIqpTqYNrmcjOoy+MslkLsZrl8ImRODXaIT5BRV8K8tuSzblsfynQVU1HiJcArjMpOYNCiFCQOSGZuZSKwGvFJhRwP9ZB3Jh+cvgooCuOkD6Dk82CVqUrXHy+q9RXyyLY9Pt+Wz+VApxtjmmuF94snqn8yEAcmcfWoqCTERwS6uUqqdNNDbomivDXVxwM0fQWJGy+/pAkqravlqXzGr9xTyxZ4i1mQXU1nrxeUQJg5K5sLhvbhwRG/6JcYEu6hKqTbQQG+rQxvgxRkQ3wtu/ABiU4JdopNW6/WxLqeYf27K5Z+bDrEzz15MPa1PD84f3pMRfRMY1juezGQ3Dof2dVeqq9NAb489/4WXvwG9R8H170BUaHcZ3JVXzj83Heafmw6zel9R/TQ27kgnp/aKZ3jveMZlJnHJ6L7ERDqDW1il1Ak00Ntr83vw+nWQNBC+8Wd70+kwUFHjYdvhcrYeKmXzwTK2Hipjy6FSiipqSY6N5LpJ/bn+zP6kxEUFu6hKKT8N9EDYvQwW3Aal+2HyD2HKPeAKv6AzxvDFniL+/MlOlmzJJTrCwezx6dxy9iAGpMYGu3hKdXsa6IFSVQIf/gy++hv0GgnfeNo2xYSp7YfLePbTXSz46gC1Ph9De8Xj9RlqvD6qa33UeH3UeHykJ8UwdVhPzhvWk7EZibicOl5NqY6igR5oWxfDwh9AZRFMuRvOmAfRCcEuVYfJLa3iL5/tYcvBMiJdDvtwOoiKcOByONhyqJRVe4rw+AwJMRGcOySN84alMWlQCn0StDeNUoGkgd4RKgph0Y9h49vgcEH/s2DoDBgyDZIHBrt0na60qpZPt+Xz7625LN2aS355DQC9ekQxJiORsZlJjMlI5PT0BNyROuBJqbbSQO9I2SthyyLY9gHkbbHL0obBoKkQkwQRMUcfrmjo0c/Oux7G0+H6fIaNB0pZvbeQNdnFfJVdzN6CCgAcAr16RNMvMYZ+STH0TYyhX2IMGcluRvbtoRdglWqBBnpnKdxtg33rYsj+HDxVjW+XcqptphkzB6LiO7eMQVJ4pIa12cWszSkmu7CS/cUV7C+u5GBxFR7f0b/BjOQYxmTY2vyYjERG9O1BdIR2n1SqjgZ6sPi89ibUtZVQW2EDfv+XsPIZOPAlRMbDmGtsuKeeEuzSBoXXZ8grq2Z3/hHW5djAX7OvmAMl9sPQ6RAGpsYytFc8Q3rFM7S3fWQmu3HqQCjVDbU70EVkGvBHwAk8Z4x5+Lj1mcBfgET/NvcYY95vbp/dItCbk7MKPv+zbYP31UL/s+HUC2DwedBrFDi6d0+R3NIqvsouZn1OCVsPl7HtcBn7CivqB0JFuhwMSHEzMDWWgalxDEqNZWBaLBlJblLjIrWnjQpb7Qp0EXEC24ALgRzgC2COMWZTg22eAb4yxjwlIqcB7xtjBjS3324f6HXKDsPql2DzQji8wS6LTbNt8KecD6dcALGpQS1iV1FR42H74XK2Hipje24Zu/Mr2J1fzr7CCmq9R/+ORSAlNoqe8VH07GGfI10Oqmt9VHt8VHu8VHt81Hp9jOibwPSRvRmTkai3+VMhob2BfibwS2PM1/2v7wUwxvxvg23+DOwyxvzGv/3vjDFnNbdfDfRGlB2Cnf+GnUvsc0U+iBMGTYFRs2HYTIjuEexSdjker4/9xZXsyj/C/qJKcsuqySurIre0msP+Z6/PEOVyEBXhJMrf9VJE2HSghFqvoU9CNNNG9mbGqD6Mz0zSeW1Ul9XeQJ8NTDPG3OJ/fR0w0RjzvQbb9AE+ApKAWOACY8zqRvY1D5gHkJmZOX7v3r1tO6PuwOeDQ2th0zuw4S0o3gfOKBhyEYycbbtHRkR3Tlnyd8B7PwRj4Ftvdd5xO0FJZS1LNh/m/fWHWLY9jxqPj9S4KEb268HA1FgGpfmbc1Jj6d0jGodDMMbg9Rk8/odTROe9UZ2mMwL9x/59/c5fQ38eGGmM8TW1X62hnwRjIOcLWP+mbXM/kgvuVMi6CSbcDPG9O+a4Pi+seBL+9WtwRkJ1KYz5Fsz6U1h2uyyv9vCvLbks2XyY7YfL2Z1/hMpab/16l7/W3rBXTp2U2Egykt1kNngMSotlRN8EDXsVUJ3R5LIRG/rZ/te7gEnGmNym9quB3kZeD+z+BFY+a7tIOlww8gqYdCv0Hdu6fdRWQu4m28TTawQk9j8xoPO2wTu32Q+SoTNg5h9g1QvwyW/g4t/BhFsCf25djDGGw6XV7Mq34b6/qBIRcDocRDgEp1OIcDio8frIKapgX6F9HCiuwusPfZdDGNYn3g6uykhiTGYifRKiqaq1bflVtT6qam2bfr/EGNLitR++al57A92FvSh6PrAfe1H0GmPMxgbbLAZeM8a8JCLDgSVAP9PMzjXQA6Bgp+0ps+YVqCmHjInQZ7SdhqDhwxkF+Vvh0Hr7yN8GDb88xSRDv/FHH7kb4d//C5FumP6Ibb8Xsc1Ar14NO/8Fcxd1ydvzdQUer4+DJVVsOVTGmmx7k5G12SWUV3tafO/gtFgmDUph0qAUJg5Kpmd8+DRvqcAIRLfFGcBj2C6JLxhjHhKRB4BVxpiF/p4tzwJxgAF+aoz5qLl9aqAHUFWJnTDsq79B6QH7mkb+XXuk28nE6h49+sKhdbB/te0fn7v56PuGzYSLf29v7tFQZTE8OxVqjsB3lnVcc0+Y8foMO/PKWbOvmIIjNURHOIiOcNpnl5NIl4MdueWs2FXAF3uK6sN/UGosp/SMo29iDH0SoumbGEPfxGj6JMSQHBupg666IR1Y1N34fLbGXlViH7WVkDIY3MnNv6+6DA6utW32A85uup388CZ47nz7oXDDe+CKDPw5AFSV2q6c/ZvtMBV2PF4fGw+U8vnuAlbuLmRfYQUHi6soa6SGHxvpJCk2kuTYSJLckcRFu6iq8VJW7aG8ykN5taf+w2F4n3hG9k1gZL8ERvVL0LtUhSgNdBV4G/4Bb95o29Iv/l3g91+w0zbv5G+DS/8Pxl0f+GOEmNKqWg4WV3GgpJJDJVUUHqmh8EgNRUdqKKywz2VVHtxRTuKiXMRFRRAX5SQu2kWtx7DpYClbD5VR47XNbfFRLgb1jCMhJsL/cNX/HB3hRAARQQQEwSF2QJc70oU70ok70klMpD1WepKO3O0szQW6Tnun2mbk5Xb6guX/Z3vcnHlb4KYQ3r0MXvcHeL8sWPQTO+FZxhlt29/BdXaitBC50XdTekRH0KN3BEN7t33+nxqPj22Hy9iwv4T1+0vYV1hBcUUN+wqOUFJZS2mVp/6C7smIj3YxcWAKZw5O4azBKQztFa+1/yDQGrpqO68H3pwLm9+FiFg72dgZ8yBtaNv3+cXzsPinkHIKzJlvPySenQq1VTBvKfToc3L72/YRzL8GImPt/vqf2faynYzC3VC0BwZP7ZzjBYgxhvJqD9UeH8aAwdhnAz5jqPb4qKjxUFnjpaLGS0WNh9JKD19lF/HZzgL2+GfVTI6N5IwByZzSM47MZDfpyTFkJLnpkxCt0zK0kza5qI51YI2dcGz9m+CttiNbz/gO9B5pk6DuQqsxtl3enXrizba9HvjwXrufUy+CK54/Oir28EZ47gLbxXLuotbf+m/XJ/DKlfYDprbSDs664lk4bVaATrwJ+dvhxelwJA9mPgZZN3bs8bqQA8WVfLazgOU7C1i1t5Ccospjavwuh9A7IZq0+ChS4+wjLS6SlLgo0uL9jzg7ZYPOm984DXTVOY7k23lpVr1g773anKgE28um7lG4C/b+F878Hlz4ADiO672xcQG8cQOMvc62qbc0sGnfCnj5ckjqbz8EwLbJZ6+EaQ/bfvsdoWivDXNvDfQ8zTYfzXoCxl7bMcfr4mq9Pg6VVLGvsIJsfz/9/cWV5JdXU1BeQ355NYVHamislSc20klafBRJsZG2vT7CRUykE3eEbbsXgWqPvQ1itcdHda23/raIHq+h1ud/9tpvG+P6JzJ9ZB/OHJxCRAh/S9BAV53L64EdH9u5aACQowFsfLbmWnrA/9gPpQdtDfrrD8G465re75IH4dNHWx7YtP9L+OssiOsJc98/2vWythLeugW2vOf/4HgwsLNalh6EF6fZrp1zF9lmo1evhl1L4fJn4fQrA3esMOL1GYoqasgrq65/5Nb9XF5NcUUNFTVeKmu8VNbaZp6KGjuCN8rlIMrf7bNujp4Ip4MIpxDhdOByCC6nA6/P8PmuAo7UeEl0R3DRab2YPqoPkwenEuly1JejqtZLVa0XAyS7I7vkdQANdBUefF54dY6dvOz6d2zXyuMd2gAvXWyba278ABL6nbiPD+6xTTsjLofLngrM3DRH8u1xS3Lg+oWQPt4ur6mAv18Fe5fD7BdgxGXtP5Zqk6paL8u25bF4wyE+3nSYsmoP0f774lbVek+Y0iHK5SAj2U1GUgyZyW4ykt307BFtRwk7BJdTcDrsh0aky0FspIu4KBexUU5io1xE+SeACzQNdBU+KottH/iCHeBOgeRBRx/xfeBfD4IjAm58v+l7uxoD//0jfHy/bc8ffwOMv7HpXjC1lXb2y/2rbW+b9PGQNPDot47KYvjLJbaL5bfeOvGDproc/nYF7F8FV/0Vhl0cqN+GaqNqj5f/bM/nPzvyEaR+oFeUyz4bY9hfXEl2YWV9c1Fj4wCa43IImclusgYkkdU/mawBSQxMjW13yGugq/BSegDWvQ5Fu23be+FuWzPG2Lnkb1wMqae2vJ89/4HPnoRti+3roTPgjG/DwK9BZRFs+9A2z+z8l73jVEPuFEifYLtVbv8IDnxle9GcekHjx6oqhZe/YQduXfiAHZSVmGnvMevUi39dnTGGkspa8sqq8fjsbJt1M27WNdVU1Hgor/ZypProgK7th8tYtbeI4opawE7iNr5/EpePS2fayLaNstZAV+GvtgqK99qpCE62P3zxPnshd/VfoLIQ4vtC+WEwXvvzsBm2Vp15pv1mkPMF5Ky2z/lbQRxw5Ust956pLIa/XW5r+nXEaUM9MdN+Q0jIaPCcCQnpre/VEyg+L/g8nX/cMOXzGXbll/PFniJW7Sli1d5CrsrK4PapbbvtpAa6Uq1RW2WnJ970DvQ6zYZ4n7HNXzitLLY9WuJ6tu4YPq/94Cned+yjaC+UZEPZwWMnTkOg3zj77WHoDOg5/MQePqUH7LeEbR/aLp6nXGDvVdtv/MlPc7zlfTsOoHQ/JA+GnsNsb520YfbYzkh7zlVF/udie51g+Ezb7NUdlefZb3BJ/Vv9Fp/PtPmCqwa6UqHCW2vDtDjbBn3hLtvkc+BLuz6xvw32gefY/v/bPrATrAEkZNrQ3f2JvSF5yql2sNfp37Q1/eYUZ8Piu2HrIhvgQ6dD3lY7YVvR7uM+ZBoR4bZNSVk3d879cEv22w+XuLSOP1ZTvLXw+dOw9GHwVMO5d8HZP+q4uY38NNCVCnWlB214b11su0F6q21TT8ZEGPJ1ewertGG2Rl5VCpsWwJpXYd9yQOwEZ5ln2ukT0iccnaitLpT+/b82tKfcA2feDs6Io8eurbQXfHO3AAaiEyEm8eizpwre+7HtfTRoClz6p46bZqFwN3zyW1g33wb6WT+AyXecOFCto+35Dyy6E/I224FwkXGw8R/QcwTM+j/77aiDaKArFU5qjtiLsD1Pa3kGzcLdsO412LLINscY/x2YkgfbcD+03s5oOWQaTP/tSTUbHMMYWP0ifHifHRQ27WHb7FPX5OPz2Salgu1Qnmu/MSQPtj2TWlOjL8mBZY/YKaIdLvtNoPwwbHjT7uP8++03kUB8O/DUwME1ENXDNqVFJx7db9kh+OjnsP51+41o+sP2G5OIba5a9GNbrjNvhyk/s/cUCDANdKXU0Q+C7JX2gm72SoiIga//Pxh+SWBuK1i4GxbcZr8ZDJpqL1Dnb4fCnbYmfzxXjG17Txlkm5Oi4u28O5GxEOn/eddS+2FhDIyfC+f85OicPvs+t1NG7F9t79g17WFIHWI/pA5v9D9vsscfMg2m/k/zH1rbPrL7K9hxdJnDZbu3xqVB4R777WjyHXD2j08M7KoS+Of9trxJA+GC+/33/41p5y/2KA10pdSJ6v7vB3rwi88Hnz9la9QxSbYtP9X/SDnV9kQq3mdDtmCXDc/Cnfbi7vHdQ8H2BBr7LdtG3VhTjs8H69+Aj38JZQeOXedOgV4jbS1+0wLbrJR1M5x7J8SmHt2uYCd8cC9s/9CO8D33p/abxpE8+43iSJ59RMbB1J/Z+ws0Z/en8O4P7DWQyHh7gX3kFbZJqp1t7BroSqnQ4PPabxL1jzKI7XniiN/G1ByxXU+N107k1nOEbTKp+8Aq2Q+fPGybbSJibS173PWw4gk7HsEVDV/7KUy8NTAXNr0e2LPM3jtg80Jbe49JguGX2jmJMia0abca6EopVSdvKyx5wA4aqzPmWtsOf/wtFwPFU2N7K214y17POOv7MPXeNu1KA10ppY6373M77mDUbEhvNB87Rk2FHbsQk9imt+sdi5RS6niZE+2js0W6gcD3fgEI3UmBlVJKHUMDXSmlwoQGulJKhQkNdKWUChMa6EopFSY00JVSKkxooCulVJjQQFdKqTARtJGiIpIH7G3j21OB/AAWpyvrLufaXc4T9FzDUWeeZ39jTKN39ghaoLeHiKxqauhruOku59pdzhP0XMNRVzlPbXJRSqkwoYGulFJhIlQD/ZlgF6ATdZdz7S7nCXqu4ahLnGdItqErpZQ6UajW0JVSSh1HA10ppcJEyAW6iEwTka0iskNE7gl2eQJJRF4QkVwR2dBgWbKI/FNEtvufk4JZxkAQkQwR+beIbBKRjSJyh395WJ2riESLyEoRWes/z1/5lw8Ukc/9f8OviUgAbmDZNYiIU0S+EpH3/K/D8lxFZI+IrBeRNSKyyr8s6H+/IRXoIuIEngCmA6cBc0TktOCWKqBeAqYdt+weYIkx5lRgif91qPMAPzHGnAZMAm73/zuG27lWA+cZY0YDY4BpIjIJ+A3wB2PMKUARcHPwihhwdwCbG7wO53OdaowZ06D/edD/fkMq0IEzgB3GmF3GmBpgPjAryGUKGGPMMqDwuMWzgL/4f/4LcFlnlqkjGGMOGmO+9P9chg2AfoTZuRqr3P8ywv8wwHnAm/7lIX+edUQkHbgYeM7/WgjTc21C0P9+Qy3Q+wHZDV7n+JeFs17GmIP+nw8BHXRb8uAQkQHAWOBzwvBc/U0Qa4Bc4J/ATqDYGOPxbxJOf8OPAT8FfP7XKYTvuRrgIxFZLSLz/MuC/verN4kOIcYYIyJh089UROKAt4AfGmNKbYXOCpdzNcZ4gTEikgi8DQwLbok6hojMBHKNMatFZEqQi9MZzjbG7BeRnsA/RWRLw5XB+vsNtRr6fiCjwet0/7JwdlhE+gD4n3ODXJ6AEJEIbJi/Yoz5h39xWJ4rgDGmGPg3cCaQKCJ1lalw+RueDFwqInuwTaHnAX8kPM8VY8x+/3Mu9oP6DLrA32+oBfoXwKn+K+eRwNXAwiCXqaMtBG7w/3wD8E4QyxIQ/rbV54HNxpjfN1gVVucqImn+mjkiEgNciL1e8G9gtn+zkD9PAGPMvcaYdGPMAOz/y38ZY64lDM9VRGJFJL7uZ+AiYANd4O835EaKisgMbFudE3jBGPNQcEsUOCLyKjAFOxXnYeB+YAHwOpCJnW74KmPM8RdOQ4qInA18CqznaHvrz7Dt6GFzriJyOvbimBNbeXrdGPOAiAzC1mKTga+AbxljqoNX0sDyN7ncaYyZGY7n6j+nt/0vXcDfjTEPiUgKQf77DblAV0op1bhQa3JRSinVBA10pZQKExroSikVJjTQlVIqTGigK6VUmNBAV0qpMKGBrpRSYeL/A+TF/FdSP6yEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('Training performance')\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 266)\n",
      "(42000, 256)\n",
      "(12000, 256)\n",
      "(6000, 256)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "X_Dataset = np.c_[X_Intensity,X_Quadrature, Label]\n",
    "\n",
    "X_Dataset_Label_shuffled = copy.copy(X_Dataset)\n",
    "np.random.shuffle(X_Dataset_Label_shuffled)\n",
    "\n",
    "length_of_X_train = int(0.7*X_Dataset_Label_shuffled.shape[0]) #42000\n",
    "length_of_X_validation = (int(0.2*X_Dataset_Label_shuffled.shape[0]))+length_of_X_train #12000\n",
    "\n",
    "\n",
    "Train_Dataset = X_Dataset_Label_shuffled[0:length_of_X_train,0:256]\n",
    "Y_Train_Dataset = X_Dataset_Label_shuffled[0:length_of_X_train,256:]\n",
    "\n",
    "Validation_Dataset = X_Dataset_Label_shuffled[length_of_X_train:length_of_X_validation,0:256]\n",
    "Y_Validation_Dataset = X_Dataset_Label_shuffled[length_of_X_train:length_of_X_validation,256:]\n",
    "\n",
    "Test_Dataset = X_Dataset_Label_shuffled[length_of_X_validation:,0:256]\n",
    "Y_Test_Dataset= X_Dataset_Label_shuffled[length_of_X_validation:,256:]\n",
    "\n",
    "print(X_Dataset_Label_shuffled.shape)\n",
    "\n",
    "print(Train_Dataset.shape)\n",
    "\n",
    "print(Validation_Dataset.shape)\n",
    "\n",
    "print(Test_Dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Experiment: Convolution 1D With I & Q \n",
      "created new ExperimentRun: Convolution 1D_IQ_drop0.1_5Layers\n"
     ]
    }
   ],
   "source": [
    "expt = client.set_experiment(\"Convolution 1D With I & Q \")\n",
    "run_2 = client.set_experiment_run(\"Convolution 1D_IQ_drop0.1_5Layers\")\n",
    "run_2.log_hyperparameter(\"num_layers\", 5)\n",
    "run_2.log_hyperparameter(\"dropout\", 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_39 (Conv1D)           (None, 247, 100)          1100      \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 238, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 79, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 70, 160)           160160    \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 61, 160)           256160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1610      \n",
      "=================================================================\n",
      "Total params: 519,130\n",
      "Trainable params: 519,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D,Conv1D,MaxPooling1D\n",
    "\n",
    "model_Dataset = Sequential()\n",
    "model_Dataset.add(Conv1D(100, 10, activation='relu', input_shape=(256,1)))\n",
    "model_Dataset.add(Conv1D(100, 10, activation='relu'))\n",
    "model_Dataset.add(MaxPooling1D(3))\n",
    "model_Dataset.add(Conv1D(160, 10, activation='relu'))\n",
    "model_Dataset.add(Conv1D(160, 10, activation='relu'))\n",
    "model_Dataset.add(GlobalMaxPooling1D())\n",
    "model_Dataset.add(Dropout(0.5))\n",
    "model_Dataset.add(Dense(10, activation='softmax'))\n",
    "print(model_Dataset.summary())\n",
    "\n",
    "model_Dataset.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Dataset=Train_Dataset.reshape(42000,256,1)\n",
    "Validation_Dataset=Validation_Dataset.reshape(12000,256,1)\n",
    "#Test_Dataset = Test_Dataset.reshape(1536000,256,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "  1/420 [..............................] - ETA: 0s - loss: 2.3028 - accuracy: 0.0600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0085s). Check your callbacks.\n",
      "418/420 [============================>.] - ETA: 0s - loss: 1.5338 - accuracy: 0.3850WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0023s). Check your callbacks.\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 1.5319 - accuracy: 0.3859 - val_loss: 1.0575 - val_accuracy: 0.5634\n",
      "Epoch 2/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 1.0278 - accuracy: 0.5683 - val_loss: 0.8277 - val_accuracy: 0.6531\n",
      "Epoch 3/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.9143 - accuracy: 0.6060 - val_loss: 0.7891 - val_accuracy: 0.6290\n",
      "Epoch 4/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.8499 - accuracy: 0.6336 - val_loss: 0.7142 - val_accuracy: 0.6893\n",
      "Epoch 5/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.8201 - accuracy: 0.6401 - val_loss: 0.6934 - val_accuracy: 0.7058\n",
      "Epoch 6/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.7843 - accuracy: 0.6531 - val_loss: 0.6819 - val_accuracy: 0.7108\n",
      "Epoch 7/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.7659 - accuracy: 0.6600 - val_loss: 0.6509 - val_accuracy: 0.7044\n",
      "Epoch 8/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.7549 - accuracy: 0.6647 - val_loss: 0.6328 - val_accuracy: 0.7136\n",
      "Epoch 9/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.7280 - accuracy: 0.6719 - val_loss: 0.6147 - val_accuracy: 0.7292\n",
      "Epoch 10/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.7225 - accuracy: 0.6759 - val_loss: 0.6412 - val_accuracy: 0.7082\n",
      "Epoch 11/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.7293 - accuracy: 0.6748 - val_loss: 0.6108 - val_accuracy: 0.7262\n",
      "Epoch 12/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.6978 - accuracy: 0.6849 - val_loss: 0.5949 - val_accuracy: 0.7263\n",
      "Epoch 13/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.6912 - accuracy: 0.6883 - val_loss: 0.6024 - val_accuracy: 0.7192\n",
      "Epoch 14/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.6847 - accuracy: 0.6881 - val_loss: 0.5969 - val_accuracy: 0.7235\n",
      "Epoch 15/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.6722 - accuracy: 0.6947 - val_loss: 0.5949 - val_accuracy: 0.7328\n",
      "Epoch 16/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.6683 - accuracy: 0.6957 - val_loss: 0.5743 - val_accuracy: 0.7387\n",
      "Epoch 17/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.6641 - accuracy: 0.6957 - val_loss: 0.5811 - val_accuracy: 0.7376\n",
      "Epoch 18/300\n",
      "420/420 [==============================] - 5s 11ms/step - loss: 0.6609 - accuracy: 0.6985 - val_loss: 0.5871 - val_accuracy: 0.7236\n",
      "Epoch 19/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6481 - accuracy: 0.7031 - val_loss: 0.5890 - val_accuracy: 0.7346\n",
      "Epoch 20/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6475 - accuracy: 0.7025 - val_loss: 0.5607 - val_accuracy: 0.7392\n",
      "Epoch 21/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6350 - accuracy: 0.7064 - val_loss: 0.5691 - val_accuracy: 0.7474\n",
      "Epoch 22/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6362 - accuracy: 0.7081 - val_loss: 0.5806 - val_accuracy: 0.7319\n",
      "Epoch 23/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6302 - accuracy: 0.7103 - val_loss: 0.5498 - val_accuracy: 0.7367\n",
      "Epoch 24/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6221 - accuracy: 0.7113 - val_loss: 0.5352 - val_accuracy: 0.7527\n",
      "Epoch 25/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6135 - accuracy: 0.7130 - val_loss: 0.5358 - val_accuracy: 0.7518\n",
      "Epoch 26/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6041 - accuracy: 0.7176 - val_loss: 0.5436 - val_accuracy: 0.7577\n",
      "Epoch 27/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6031 - accuracy: 0.7185 - val_loss: 0.5287 - val_accuracy: 0.7525\n",
      "Epoch 28/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.6060 - accuracy: 0.7181 - val_loss: 0.5264 - val_accuracy: 0.7542\n",
      "Epoch 29/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5889 - accuracy: 0.7242 - val_loss: 0.5221 - val_accuracy: 0.7537\n",
      "Epoch 30/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5852 - accuracy: 0.7255 - val_loss: 0.5258 - val_accuracy: 0.7488\n",
      "Epoch 31/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5841 - accuracy: 0.7290 - val_loss: 0.5199 - val_accuracy: 0.7588\n",
      "Epoch 32/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5837 - accuracy: 0.7280 - val_loss: 0.5541 - val_accuracy: 0.7372\n",
      "Epoch 33/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5719 - accuracy: 0.7318 - val_loss: 0.5174 - val_accuracy: 0.7601\n",
      "Epoch 34/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5699 - accuracy: 0.7320 - val_loss: 0.5200 - val_accuracy: 0.7537\n",
      "Epoch 35/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5680 - accuracy: 0.7338 - val_loss: 0.5289 - val_accuracy: 0.7577\n",
      "Epoch 36/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5643 - accuracy: 0.7343 - val_loss: 0.4983 - val_accuracy: 0.7678\n",
      "Epoch 37/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5524 - accuracy: 0.7399 - val_loss: 0.4882 - val_accuracy: 0.7713\n",
      "Epoch 38/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5500 - accuracy: 0.7425 - val_loss: 0.4976 - val_accuracy: 0.7713\n",
      "Epoch 39/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5457 - accuracy: 0.7429 - val_loss: 0.4901 - val_accuracy: 0.7692\n",
      "Epoch 40/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5468 - accuracy: 0.7432 - val_loss: 0.5239 - val_accuracy: 0.7513\n",
      "Epoch 41/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5409 - accuracy: 0.7439 - val_loss: 0.4950 - val_accuracy: 0.7563\n",
      "Epoch 42/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5348 - accuracy: 0.7468 - val_loss: 0.4871 - val_accuracy: 0.7692\n",
      "Epoch 43/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5351 - accuracy: 0.7453 - val_loss: 0.4795 - val_accuracy: 0.7747\n",
      "Epoch 44/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5319 - accuracy: 0.7469 - val_loss: 0.4746 - val_accuracy: 0.7661\n",
      "Epoch 45/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5270 - accuracy: 0.7484 - val_loss: 0.4779 - val_accuracy: 0.7747\n",
      "Epoch 46/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5232 - accuracy: 0.7515 - val_loss: 0.4982 - val_accuracy: 0.7677\n",
      "Epoch 47/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5209 - accuracy: 0.7523 - val_loss: 0.4662 - val_accuracy: 0.7769\n",
      "Epoch 48/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5159 - accuracy: 0.7541 - val_loss: 0.4826 - val_accuracy: 0.7718\n",
      "Epoch 49/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5152 - accuracy: 0.7559 - val_loss: 0.4778 - val_accuracy: 0.7725\n",
      "Epoch 50/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5079 - accuracy: 0.7572 - val_loss: 0.4691 - val_accuracy: 0.7782\n",
      "Epoch 51/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5043 - accuracy: 0.7594 - val_loss: 0.4689 - val_accuracy: 0.7758\n",
      "Epoch 52/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5077 - accuracy: 0.7566 - val_loss: 0.4590 - val_accuracy: 0.7726\n",
      "Epoch 53/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.5015 - accuracy: 0.7602 - val_loss: 0.4943 - val_accuracy: 0.7727\n",
      "Epoch 54/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4953 - accuracy: 0.7620 - val_loss: 0.4633 - val_accuracy: 0.7746\n",
      "Epoch 55/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4974 - accuracy: 0.7620 - val_loss: 0.4633 - val_accuracy: 0.7756\n",
      "Epoch 56/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4910 - accuracy: 0.7630 - val_loss: 0.4564 - val_accuracy: 0.7754\n",
      "Epoch 57/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4874 - accuracy: 0.7654 - val_loss: 0.4671 - val_accuracy: 0.7832\n",
      "Epoch 58/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4891 - accuracy: 0.7648 - val_loss: 0.4544 - val_accuracy: 0.7866\n",
      "Epoch 59/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4763 - accuracy: 0.7688 - val_loss: 0.4450 - val_accuracy: 0.7873\n",
      "Epoch 60/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4794 - accuracy: 0.7677 - val_loss: 0.4375 - val_accuracy: 0.7842\n",
      "Epoch 61/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4795 - accuracy: 0.7692 - val_loss: 0.4407 - val_accuracy: 0.7886\n",
      "Epoch 62/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4760 - accuracy: 0.7690 - val_loss: 0.4467 - val_accuracy: 0.7878\n",
      "Epoch 63/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4763 - accuracy: 0.7692 - val_loss: 0.4466 - val_accuracy: 0.7800\n",
      "Epoch 64/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4681 - accuracy: 0.7736 - val_loss: 0.4435 - val_accuracy: 0.7821\n",
      "Epoch 65/300\n",
      "420/420 [==============================] - 5s 12ms/step - loss: 0.4678 - accuracy: 0.7732 - val_loss: 0.4386 - val_accuracy: 0.7907\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "history_Dataset = model_Dataset.fit(Train_Dataset,\n",
    "                      Y_Train_Dataset,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      validation_data=(Validation_Dataset, Y_Validation_Dataset),\n",
    "                    callbacks =[tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4386 - accuracy: 0.7907\n",
      "Accuracy: 79.07\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model_Dataset.evaluate(Validation_Dataset, Y_Validation_Dataset)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "run_2.log_metric(\"accuracy\", accuracy)\n",
    "run_2.log_tags([\"Experiment_IQ_dropout0.1_5Layers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/DATA/projects/signals/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /mnt/DATA/projects/signals/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: Convolution 1D_IQ_drop0.1_5Layers/assets\n"
     ]
    }
   ],
   "source": [
    "model_Dataset.save(\"Convolution 1D_IQ_drop0.1_5Layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5, 0, 9, 0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "loadedmodel = keras.models.load_model(\"Convolution 1D_IQ_drop0.1_5Layers\")\n",
    "np.argmax(loadedmodel.predict(Validation_Dataset[:5]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('Training performance')\n",
    "plt.plot(history_Dataset.history['loss'], label='train_loss')\n",
    "plt.plot(history_Dataset.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os,random\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"THEANO_FLAGS\"]  = \"device=gpu%d\"%(1)\n",
    "import numpy as np\n",
    "# import theano as th\n",
    "# import theano.tensor as T\n",
    "from keras.utils import np_utils\n",
    "import keras.models as models\n",
    "from keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as cPickle, random, sys, keras\n",
    "\n",
    "\n",
    "in_shp = [2,128]\n",
    "print(Train_Dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4wave = keras.Sequential()\n",
    "model_4wave.add(\n",
    "    keras.layers.Bidirectional(\n",
    "      keras.layers.LSTM(\n",
    "          units=128, \n",
    "          input_shape=[256, 1]\n",
    "      )\n",
    "    )\n",
    ")\n",
    "model_4wave.add(keras.layers.Dropout(rate=0.2))\n",
    "model_4wave.add(keras.layers.Dense(units=128, activation='relu'))\n",
    "model_4wave.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model_4wave.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_4wave.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Dataset=Train_Dataset.reshape(42000,256,1)\n",
    "Validation_Dataset=Validation_Dataset.reshape(12000,256,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/420 [..............................] - ETA: 4s - loss: 2.3025 - acc: 0.1200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0091s vs `on_train_batch_end` time: 0.0344s). Check your callbacks.\n",
      "420/420 [==============================] - 12s 28ms/step - loss: 2.2988 - acc: 0.1070\n",
      "Epoch 2/10\n",
      "420/420 [==============================] - 12s 28ms/step - loss: 2.2875 - acc: 0.1250\n",
      "Epoch 3/10\n",
      "420/420 [==============================] - 12s 28ms/step - loss: 2.3025 - acc: 0.1341\n",
      "Epoch 4/10\n",
      "420/420 [==============================] - 12s 29ms/step - loss: 2.2377 - acc: 0.1445\n",
      "Epoch 5/10\n",
      "420/420 [==============================] - 12s 29ms/step - loss: 2.1049 - acc: 0.1700\n",
      "Epoch 6/10\n",
      "420/420 [==============================] - 12s 29ms/step - loss: 2.0156 - acc: 0.1876\n",
      "Epoch 7/10\n",
      "420/420 [==============================] - 12s 29ms/step - loss: 2.0887 - acc: 0.1736\n",
      "Epoch 8/10\n",
      "420/420 [==============================] - 12s 29ms/step - loss: 2.0766 - acc: 0.1754\n",
      "Epoch 9/10\n",
      "420/420 [==============================] - 12s 29ms/step - loss: 2.0028 - acc: 0.1947\n",
      "Epoch 10/10\n",
      "420/420 [==============================] - 12s 29ms/step - loss: 1.9906 - acc: 0.1934\n"
     ]
    }
   ],
   "source": [
    "history = model_4wave.fit(\n",
    "    Train_Dataset,Y_Train_Dataset ,\n",
    "    epochs=10,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 256, 1)    0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ReStk0_conv1 (Conv2D)           (None, 32, 256, 1)   64          reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ReStk0_conv2 (Conv2D)           (None, 32, 256, 1)   6176        ReStk0_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ReStk0_conv3 (Conv2D)           (None, 32, 256, 1)   6176        ReStk0_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 32, 256, 1)   0           ReStk0_conv3[0][0]               \n",
      "                                                                 ReStk0_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 32, 256, 1)   0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ReStk0_conv4 (Conv2D)           (None, 32, 256, 1)   6176        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 32, 256, 1)   0           ReStk0_conv4[0][0]               \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 32, 256, 1)   0           add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D) (None, 32, 128, 1)   0           activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ReStk1_conv1 (Conv2D)           (None, 32, 128, 1)   1056        max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ReStk1_conv2 (Conv2D)           (None, 32, 128, 1)   3104        ReStk1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ReStk1_conv3 (Conv2D)           (None, 32, 128, 1)   3104        ReStk1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 32, 128, 1)   0           ReStk1_conv3[0][0]               \n",
      "                                                                 ReStk1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 32, 128, 1)   0           add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ReStk1_conv4 (Conv2D)           (None, 32, 128, 1)   3104        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 32, 128, 1)   0           ReStk1_conv4[0][0]               \n",
      "                                                                 activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 128, 1)   0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D) (None, 32, 64, 1)    0           activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ReStk2_conv1 (Conv2D)           (None, 32, 64, 1)    1056        max_pooling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ReStk2_conv2 (Conv2D)           (None, 32, 64, 1)    3104        ReStk2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ReStk2_conv3 (Conv2D)           (None, 32, 64, 1)    3104        ReStk2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 32, 64, 1)    0           ReStk2_conv3[0][0]               \n",
      "                                                                 ReStk2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 64, 1)    0           add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ReStk2_conv4 (Conv2D)           (None, 32, 64, 1)    3104        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 32, 64, 1)    0           ReStk2_conv4[0][0]               \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 64, 1)    0           add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 32, 32, 1)    0           activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ReStk3_conv1 (Conv2D)           (None, 32, 32, 1)    1056        max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ReStk3_conv2 (Conv2D)           (None, 32, 32, 1)    3104        ReStk3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ReStk3_conv3 (Conv2D)           (None, 32, 32, 1)    3104        ReStk3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 32, 32, 1)    0           ReStk3_conv3[0][0]               \n",
      "                                                                 ReStk3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 32, 32, 1)    0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ReStk3_conv4 (Conv2D)           (None, 32, 32, 1)    3104        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 32, 32, 1)    0           ReStk3_conv4[0][0]               \n",
      "                                                                 activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 32, 32, 1)    0           add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 32, 16, 1)    0           activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ReStk4_conv1 (Conv2D)           (None, 32, 16, 1)    1056        max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ReStk4_conv2 (Conv2D)           (None, 32, 16, 1)    3104        ReStk4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ReStk4_conv3 (Conv2D)           (None, 32, 16, 1)    3104        ReStk4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 32, 16, 1)    0           ReStk4_conv3[0][0]               \n",
      "                                                                 ReStk4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 16, 1)    0           add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ReStk4_conv4 (Conv2D)           (None, 32, 16, 1)    3104        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 32, 16, 1)    0           ReStk4_conv4[0][0]               \n",
      "                                                                 activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 32, 16, 1)    0           add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling2D) (None, 32, 8, 1)     0           activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ReStk5_conv1 (Conv2D)           (None, 32, 8, 1)     1056        max_pooling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ReStk5_conv2 (Conv2D)           (None, 32, 8, 1)     3104        ReStk5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ReStk5_conv3 (Conv2D)           (None, 32, 8, 1)     3104        ReStk5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 32, 8, 1)     0           ReStk5_conv3[0][0]               \n",
      "                                                                 ReStk5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 32, 8, 1)     0           add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ReStk5_conv4 (Conv2D)           (None, 32, 8, 1)     3104        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 32, 8, 1)     0           ReStk5_conv4[0][0]               \n",
      "                                                                 activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 32, 8, 1)     0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling2D) (None, 32, 4, 1)     0           activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 128)          0           max_pooling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 128)          16512       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_5 (AlphaDropout)  (None, 128)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 10)           1290        alpha_dropout_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 10)           0           dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 88,234\n",
      "Trainable params: 88,234\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Reshape,ZeroPadding2D,Conv2D,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout\n",
    "import numpy as np\n",
    "import os,random\n",
    "from keras.layers import Input,Reshape,ZeroPadding2D,Conv2D,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout\n",
    "from keras import layers\n",
    "import keras.models as Model\n",
    "from keras.regularizers import *\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%matplotlib inline\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "data_format = 'channels_first'\n",
    "\n",
    "classes=[1,2,3,4,5,6,7,8,9,10]\n",
    "def residual_stack(Xm,kennel_size,Seq,pool_size):\n",
    "    #1*1 Conv Linear\n",
    "    Xm = Conv2D(32, (1, 1), padding='same', name=Seq+\"_conv1\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n",
    "    #Residual Unit 1\n",
    "    Xm_shortcut = Xm\n",
    "    Xm = Conv2D(32, kennel_size, padding='same',activation=\"relu\",name=Seq+\"_conv2\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n",
    "    Xm = Conv2D(32, kennel_size, padding='same', name=Seq+\"_conv3\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n",
    "    Xm = layers.add([Xm,Xm_shortcut])\n",
    "    Xm = Activation(\"relu\")(Xm)\n",
    "    #Residual Unit 2\n",
    "    Xm_shortcut = Xm\n",
    "    Xm = Conv2D(32, kennel_size, padding='same',activation=\"relu\",name=Seq+\"_conv4\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n",
    "    X = Conv2D(32, kennel_size, padding='same', name=Seq+\"_conv5\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n",
    "    Xm = layers.add([Xm,Xm_shortcut])\n",
    "    Xm = Activation(\"relu\")(Xm)\n",
    "    #MaxPooling\n",
    "    Xm = MaxPooling2D(pool_size=pool_size, strides=pool_size, padding='valid', data_format=data_format)(Xm)\n",
    "    return Xm\n",
    "\n",
    "\n",
    "in_shp = Train_Dataset.shape[1:] \n",
    "\n",
    "\n",
    "#input layer\n",
    "\n",
    "Xm_input = Input(in_shp)\n",
    "Xm = Reshape([1,256,1], input_shape=in_shp)(Xm_input)\n",
    "\n",
    "#Residual Srack\n",
    "Xm = residual_stack(Xm,kennel_size=(3,2),Seq=\"ReStk0\",pool_size=(2,1)) \n",
    "Xm = residual_stack(Xm,kennel_size=(3,1),Seq=\"ReStk1\",pool_size=(2,1))\n",
    "Xm = residual_stack(Xm,kennel_size=(3,1),Seq=\"ReStk2\",pool_size=(2,1))  \n",
    "Xm = residual_stack(Xm,kennel_size=(3,1),Seq=\"ReStk3\",pool_size=(2,1)) \n",
    "Xm = residual_stack(Xm,kennel_size=(3,1),Seq=\"ReStk4\",pool_size=(2,1)) \n",
    "Xm = residual_stack(Xm,kennel_size=(3,1),Seq=\"ReStk5\",pool_size=(2,1))   \n",
    "\n",
    "\n",
    "#Full Con 1\n",
    "Xm = Flatten(data_format=data_format)(Xm)\n",
    "Xm = Dense(128, activation='selu', kernel_initializer='glorot_normal', name=\"dense1\")(Xm)\n",
    "Xm = AlphaDropout(0.3)(Xm)\n",
    "#Full Con 2\n",
    "Xm = Dense(len(classes), kernel_initializer='glorot_normal', name=\"dense2\")(Xm)\n",
    "#SoftMax\n",
    "Xm = Activation('softmax')(Xm)\n",
    "#Create Model\n",
    "model = Model.Model(inputs=Xm_input,outputs=Xm)\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics='accuracy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 1.4509 - accuracy: 0.3987\n",
      "Epoch 2/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.9739 - accuracy: 0.5693\n",
      "Epoch 3/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.8937 - accuracy: 0.6011\n",
      "Epoch 4/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.8362 - accuracy: 0.6256\n",
      "Epoch 5/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.7813 - accuracy: 0.6473\n",
      "Epoch 6/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.7519 - accuracy: 0.6624\n",
      "Epoch 7/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.7301 - accuracy: 0.6692\n",
      "Epoch 8/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.8753 - accuracy: 0.6221\n",
      "Epoch 9/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.7378 - accuracy: 0.6707\n",
      "Epoch 10/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.7302 - accuracy: 0.6748\n",
      "Epoch 11/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.7242 - accuracy: 0.6772\n",
      "Epoch 12/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.6698 - accuracy: 0.6948\n",
      "Epoch 13/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.6379 - accuracy: 0.7071\n",
      "Epoch 14/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.6077 - accuracy: 0.7186\n",
      "Epoch 15/600\n",
      "420/420 [==============================] - ETA: 0s - loss: 0.5930 - accuracy: 0.72 - 3s 7ms/step - loss: 0.5938 - accuracy: 0.7232\n",
      "Epoch 16/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.5693 - accuracy: 0.7307\n",
      "Epoch 17/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.5559 - accuracy: 0.7354\n",
      "Epoch 18/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.5462 - accuracy: 0.7407\n",
      "Epoch 19/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.5351 - accuracy: 0.7438\n",
      "Epoch 20/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.5311 - accuracy: 0.7487\n",
      "Epoch 21/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.5461 - accuracy: 0.7421\n",
      "Epoch 22/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4936 - accuracy: 0.7581\n",
      "Epoch 23/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.5451 - accuracy: 0.7460\n",
      "Epoch 24/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4739 - accuracy: 0.7656\n",
      "Epoch 25/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4685 - accuracy: 0.7701\n",
      "Epoch 26/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.4576 - accuracy: 0.7724\n",
      "Epoch 27/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4568 - accuracy: 0.7744\n",
      "Epoch 28/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.4455 - accuracy: 0.7782\n",
      "Epoch 29/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4413 - accuracy: 0.7819\n",
      "Epoch 30/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4688 - accuracy: 0.7709\n",
      "Epoch 31/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4442 - accuracy: 0.7805\n",
      "Epoch 32/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4287 - accuracy: 0.7845\n",
      "Epoch 33/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4212 - accuracy: 0.7856\n",
      "Epoch 34/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.4133 - accuracy: 0.7910\n",
      "Epoch 35/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.4631 - accuracy: 0.7764\n",
      "Epoch 36/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4118 - accuracy: 0.7932\n",
      "Epoch 37/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4032 - accuracy: 0.7943\n",
      "Epoch 38/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4118 - accuracy: 0.7951\n",
      "Epoch 39/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3860 - accuracy: 0.8008\n",
      "Epoch 40/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3947 - accuracy: 0.7981\n",
      "Epoch 41/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3803 - accuracy: 0.8056\n",
      "Epoch 42/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4418 - accuracy: 0.7844\n",
      "Epoch 43/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3888 - accuracy: 0.8004\n",
      "Epoch 44/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3811 - accuracy: 0.8050\n",
      "Epoch 45/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3952 - accuracy: 0.8022\n",
      "Epoch 46/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3680 - accuracy: 0.8109\n",
      "Epoch 47/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3620 - accuracy: 0.8133\n",
      "Epoch 48/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3775 - accuracy: 0.8084\n",
      "Epoch 49/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3717 - accuracy: 0.8089\n",
      "Epoch 50/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3951 - accuracy: 0.8033\n",
      "Epoch 51/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.4798 - accuracy: 0.7795\n",
      "Epoch 52/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.4381 - accuracy: 0.7856\n",
      "Epoch 53/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3593 - accuracy: 0.8155\n",
      "Epoch 54/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3424 - accuracy: 0.8199\n",
      "Epoch 55/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3513 - accuracy: 0.8164\n",
      "Epoch 56/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3478 - accuracy: 0.8200\n",
      "Epoch 57/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3469 - accuracy: 0.8204\n",
      "Epoch 58/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3523 - accuracy: 0.8171\n",
      "Epoch 59/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3463 - accuracy: 0.8219\n",
      "Epoch 60/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3581 - accuracy: 0.8180\n",
      "Epoch 61/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3422 - accuracy: 0.8230\n",
      "Epoch 62/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3435 - accuracy: 0.8219\n",
      "Epoch 63/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3574 - accuracy: 0.8186\n",
      "Epoch 64/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3349 - accuracy: 0.8252\n",
      "Epoch 65/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3425 - accuracy: 0.8238\n",
      "Epoch 66/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3328 - accuracy: 0.8261\n",
      "Epoch 67/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3260 - accuracy: 0.8305\n",
      "Epoch 68/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3383 - accuracy: 0.8286\n",
      "Epoch 69/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3539 - accuracy: 0.8230\n",
      "Epoch 70/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3213 - accuracy: 0.8325\n",
      "Epoch 71/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3285 - accuracy: 0.8291\n",
      "Epoch 72/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3155 - accuracy: 0.8376\n",
      "Epoch 73/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3148 - accuracy: 0.8363\n",
      "Epoch 74/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3299 - accuracy: 0.8289\n",
      "Epoch 75/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3201 - accuracy: 0.8336\n",
      "Epoch 76/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3111 - accuracy: 0.8385\n",
      "Epoch 77/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3164 - accuracy: 0.8363\n",
      "Epoch 78/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3098 - accuracy: 0.8393\n",
      "Epoch 79/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3156 - accuracy: 0.8385\n",
      "Epoch 80/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3034 - accuracy: 0.8420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3044 - accuracy: 0.8418\n",
      "Epoch 82/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3073 - accuracy: 0.8419\n",
      "Epoch 83/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3189 - accuracy: 0.8363\n",
      "Epoch 84/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2950 - accuracy: 0.8457\n",
      "Epoch 85/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3071 - accuracy: 0.8413\n",
      "Epoch 86/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2975 - accuracy: 0.8450\n",
      "Epoch 87/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3059 - accuracy: 0.8416\n",
      "Epoch 88/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3000 - accuracy: 0.8440\n",
      "Epoch 89/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3130 - accuracy: 0.8397\n",
      "Epoch 90/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3072 - accuracy: 0.8420\n",
      "Epoch 91/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.3022 - accuracy: 0.8471\n",
      "Epoch 92/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2771 - accuracy: 0.8550\n",
      "Epoch 93/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2972 - accuracy: 0.8444\n",
      "Epoch 94/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2875 - accuracy: 0.8505\n",
      "Epoch 95/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.3031 - accuracy: 0.8431\n",
      "Epoch 96/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2913 - accuracy: 0.8502\n",
      "Epoch 97/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2846 - accuracy: 0.8516\n",
      "Epoch 98/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2838 - accuracy: 0.8507\n",
      "Epoch 99/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2883 - accuracy: 0.8523\n",
      "Epoch 100/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2820 - accuracy: 0.8530\n",
      "Epoch 101/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2909 - accuracy: 0.8500\n",
      "Epoch 102/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2849 - accuracy: 0.8536\n",
      "Epoch 103/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2699 - accuracy: 0.8573\n",
      "Epoch 104/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2867 - accuracy: 0.8510\n",
      "Epoch 105/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2726 - accuracy: 0.8571\n",
      "Epoch 106/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2734 - accuracy: 0.8588\n",
      "Epoch 107/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2967 - accuracy: 0.8478\n",
      "Epoch 108/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2700 - accuracy: 0.8571\n",
      "Epoch 109/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2676 - accuracy: 0.8607\n",
      "Epoch 110/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2735 - accuracy: 0.8575\n",
      "Epoch 111/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2729 - accuracy: 0.8560\n",
      "Epoch 112/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2692 - accuracy: 0.8575\n",
      "Epoch 113/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2731 - accuracy: 0.8586\n",
      "Epoch 114/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2618 - accuracy: 0.8634\n",
      "Epoch 115/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2753 - accuracy: 0.8585\n",
      "Epoch 116/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2705 - accuracy: 0.8626\n",
      "Epoch 117/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2516 - accuracy: 0.8685\n",
      "Epoch 118/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2647 - accuracy: 0.8640\n",
      "Epoch 119/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2783 - accuracy: 0.8564\n",
      "Epoch 120/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2612 - accuracy: 0.8647\n",
      "Epoch 121/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2666 - accuracy: 0.8624\n",
      "Epoch 122/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2651 - accuracy: 0.8641\n",
      "Epoch 123/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2680 - accuracy: 0.8620\n",
      "Epoch 124/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2539 - accuracy: 0.8668\n",
      "Epoch 125/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2518 - accuracy: 0.8665\n",
      "Epoch 126/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2760 - accuracy: 0.8612\n",
      "Epoch 127/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2513 - accuracy: 0.8684\n",
      "Epoch 128/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2562 - accuracy: 0.8663\n",
      "Epoch 129/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2654 - accuracy: 0.8629\n",
      "Epoch 130/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2555 - accuracy: 0.8678\n",
      "Epoch 131/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2433 - accuracy: 0.8726\n",
      "Epoch 132/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2544 - accuracy: 0.8670\n",
      "Epoch 133/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2581 - accuracy: 0.8678\n",
      "Epoch 134/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2435 - accuracy: 0.8731\n",
      "Epoch 135/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2526 - accuracy: 0.8680\n",
      "Epoch 136/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2675 - accuracy: 0.8638\n",
      "Epoch 137/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2383 - accuracy: 0.8736\n",
      "Epoch 138/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2447 - accuracy: 0.8717\n",
      "Epoch 139/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2404 - accuracy: 0.8745\n",
      "Epoch 140/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2382 - accuracy: 0.8771\n",
      "Epoch 141/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2596 - accuracy: 0.8680\n",
      "Epoch 142/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2438 - accuracy: 0.8748\n",
      "Epoch 143/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2390 - accuracy: 0.8771\n",
      "Epoch 144/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2408 - accuracy: 0.8746\n",
      "Epoch 145/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2458 - accuracy: 0.8730\n",
      "Epoch 146/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2353 - accuracy: 0.8766\n",
      "Epoch 147/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2394 - accuracy: 0.8773\n",
      "Epoch 148/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2481 - accuracy: 0.8724\n",
      "Epoch 149/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2589 - accuracy: 0.8682\n",
      "Epoch 150/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2342 - accuracy: 0.8765\n",
      "Epoch 151/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2324 - accuracy: 0.8783\n",
      "Epoch 152/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2512 - accuracy: 0.8736\n",
      "Epoch 153/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2522 - accuracy: 0.8730\n",
      "Epoch 154/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2260 - accuracy: 0.8811\n",
      "Epoch 155/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2477 - accuracy: 0.8738\n",
      "Epoch 156/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2300 - accuracy: 0.8800\n",
      "Epoch 157/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2375 - accuracy: 0.8776\n",
      "Epoch 158/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2337 - accuracy: 0.8798\n",
      "Epoch 159/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2245 - accuracy: 0.8835\n",
      "Epoch 160/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2443 - accuracy: 0.8743\n",
      "Epoch 161/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2352 - accuracy: 0.8797\n",
      "Epoch 162/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2395 - accuracy: 0.8781\n",
      "Epoch 163/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2204 - accuracy: 0.8848\n",
      "Epoch 164/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2416 - accuracy: 0.8763\n",
      "Epoch 165/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2476 - accuracy: 0.8750\n",
      "Epoch 166/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2182 - accuracy: 0.8854\n",
      "Epoch 167/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2212 - accuracy: 0.8840\n",
      "Epoch 168/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2249 - accuracy: 0.8830\n",
      "Epoch 169/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2306 - accuracy: 0.8819\n",
      "Epoch 170/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2388 - accuracy: 0.8778\n",
      "Epoch 171/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2216 - accuracy: 0.8856\n",
      "Epoch 172/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2296 - accuracy: 0.8813\n",
      "Epoch 173/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2278 - accuracy: 0.8827\n",
      "Epoch 174/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2325 - accuracy: 0.8821\n",
      "Epoch 175/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2328 - accuracy: 0.8825\n",
      "Epoch 176/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2193 - accuracy: 0.8868\n",
      "Epoch 177/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2310 - accuracy: 0.8837\n",
      "Epoch 178/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2157 - accuracy: 0.8875\n",
      "Epoch 179/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2236 - accuracy: 0.8856\n",
      "Epoch 180/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2273 - accuracy: 0.8845\n",
      "Epoch 181/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2204 - accuracy: 0.8869\n",
      "Epoch 182/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2105 - accuracy: 0.8908\n",
      "Epoch 183/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2300 - accuracy: 0.8839\n",
      "Epoch 184/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2401 - accuracy: 0.8807\n",
      "Epoch 185/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2174 - accuracy: 0.8877\n",
      "Epoch 186/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2226 - accuracy: 0.8865\n",
      "Epoch 187/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2212 - accuracy: 0.8866\n",
      "Epoch 188/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2172 - accuracy: 0.8867\n",
      "Epoch 189/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2311 - accuracy: 0.8840\n",
      "Epoch 190/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2085 - accuracy: 0.8917\n",
      "Epoch 191/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2166 - accuracy: 0.8876\n",
      "Epoch 192/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2142 - accuracy: 0.8916\n",
      "Epoch 193/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2278 - accuracy: 0.8833\n",
      "Epoch 194/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2055 - accuracy: 0.8928\n",
      "Epoch 195/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2238 - accuracy: 0.8847\n",
      "Epoch 196/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2187 - accuracy: 0.8885\n",
      "Epoch 197/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2079 - accuracy: 0.8910\n",
      "Epoch 198/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2078 - accuracy: 0.8941\n",
      "Epoch 199/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2174 - accuracy: 0.8885\n",
      "Epoch 200/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2154 - accuracy: 0.8896\n",
      "Epoch 201/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1993 - accuracy: 0.8956\n",
      "Epoch 202/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2150 - accuracy: 0.8909\n",
      "Epoch 203/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2332 - accuracy: 0.8824\n",
      "Epoch 204/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2185 - accuracy: 0.8900\n",
      "Epoch 205/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2163 - accuracy: 0.8908\n",
      "Epoch 206/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2186 - accuracy: 0.8885\n",
      "Epoch 207/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2089 - accuracy: 0.8926\n",
      "Epoch 208/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2243 - accuracy: 0.8866\n",
      "Epoch 209/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2084 - accuracy: 0.8929\n",
      "Epoch 210/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2168 - accuracy: 0.8925\n",
      "Epoch 211/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2182 - accuracy: 0.8903\n",
      "Epoch 212/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2088 - accuracy: 0.8921\n",
      "Epoch 213/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2225 - accuracy: 0.8868\n",
      "Epoch 214/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2113 - accuracy: 0.8935\n",
      "Epoch 215/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2103 - accuracy: 0.8923\n",
      "Epoch 216/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2211 - accuracy: 0.8901\n",
      "Epoch 217/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2153 - accuracy: 0.8923\n",
      "Epoch 218/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2038 - accuracy: 0.8988\n",
      "Epoch 219/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2091 - accuracy: 0.8928\n",
      "Epoch 220/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2133 - accuracy: 0.8912\n",
      "Epoch 221/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2042 - accuracy: 0.8950\n",
      "Epoch 222/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2108 - accuracy: 0.8923\n",
      "Epoch 223/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2146 - accuracy: 0.8913\n",
      "Epoch 224/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2150 - accuracy: 0.8919\n",
      "Epoch 225/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2069 - accuracy: 0.8953\n",
      "Epoch 226/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2020 - accuracy: 0.8947\n",
      "Epoch 227/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2149 - accuracy: 0.8922\n",
      "Epoch 228/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2037 - accuracy: 0.8957\n",
      "Epoch 229/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1979 - accuracy: 0.8984\n",
      "Epoch 230/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2059 - accuracy: 0.8963\n",
      "Epoch 231/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2109 - accuracy: 0.8930\n",
      "Epoch 232/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1920 - accuracy: 0.9011\n",
      "Epoch 233/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2103 - accuracy: 0.8940\n",
      "Epoch 234/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2054 - accuracy: 0.8936\n",
      "Epoch 235/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1997 - accuracy: 0.8980\n",
      "Epoch 236/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1980 - accuracy: 0.8976\n",
      "Epoch 237/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2117 - accuracy: 0.8925\n",
      "Epoch 238/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1959 - accuracy: 0.8995\n",
      "Epoch 239/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2155 - accuracy: 0.8894\n",
      "Epoch 240/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1986 - accuracy: 0.8988\n",
      "Epoch 241/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1923 - accuracy: 0.9009\n",
      "Epoch 242/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2039 - accuracy: 0.8956\n",
      "Epoch 243/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1882 - accuracy: 0.9025\n",
      "Epoch 244/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2020 - accuracy: 0.8970\n",
      "Epoch 245/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1990 - accuracy: 0.8969\n",
      "Epoch 246/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2201 - accuracy: 0.8912\n",
      "Epoch 247/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1920 - accuracy: 0.9009\n",
      "Epoch 248/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1917 - accuracy: 0.9012\n",
      "Epoch 249/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2061 - accuracy: 0.8970\n",
      "Epoch 250/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2072 - accuracy: 0.8958\n",
      "Epoch 251/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2069 - accuracy: 0.8964\n",
      "Epoch 252/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2025 - accuracy: 0.8973\n",
      "Epoch 253/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1989 - accuracy: 0.8994\n",
      "Epoch 254/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2038 - accuracy: 0.8967\n",
      "Epoch 255/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1848 - accuracy: 0.9046\n",
      "Epoch 256/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1957 - accuracy: 0.8996\n",
      "Epoch 257/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2076 - accuracy: 0.8969\n",
      "Epoch 258/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1948 - accuracy: 0.8988\n",
      "Epoch 259/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1990 - accuracy: 0.8987\n",
      "Epoch 260/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2045 - accuracy: 0.8973\n",
      "Epoch 261/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1960 - accuracy: 0.9005\n",
      "Epoch 262/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1943 - accuracy: 0.9013\n",
      "Epoch 263/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2036 - accuracy: 0.8985\n",
      "Epoch 264/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1880 - accuracy: 0.9021\n",
      "Epoch 265/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1793 - accuracy: 0.9062\n",
      "Epoch 266/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1990 - accuracy: 0.8992\n",
      "Epoch 267/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2053 - accuracy: 0.8968\n",
      "Epoch 268/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2051 - accuracy: 0.8977\n",
      "Epoch 269/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1865 - accuracy: 0.9030\n",
      "Epoch 270/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2008 - accuracy: 0.8983\n",
      "Epoch 271/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1888 - accuracy: 0.9020\n",
      "Epoch 272/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1884 - accuracy: 0.9034\n",
      "Epoch 273/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.2016 - accuracy: 0.8995\n",
      "Epoch 274/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1875 - accuracy: 0.9043\n",
      "Epoch 275/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1980 - accuracy: 0.9007\n",
      "Epoch 276/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1948 - accuracy: 0.9006\n",
      "Epoch 277/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1967 - accuracy: 0.8991\n",
      "Epoch 278/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1927 - accuracy: 0.9030\n",
      "Epoch 279/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1890 - accuracy: 0.9032\n",
      "Epoch 280/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1917 - accuracy: 0.9025\n",
      "Epoch 281/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1997 - accuracy: 0.8999\n",
      "Epoch 282/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1847 - accuracy: 0.9039\n",
      "Epoch 283/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1995 - accuracy: 0.8996\n",
      "Epoch 284/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1905 - accuracy: 0.9011\n",
      "Epoch 285/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1854 - accuracy: 0.9048\n",
      "Epoch 286/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1840 - accuracy: 0.9057\n",
      "Epoch 287/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1852 - accuracy: 0.9055\n",
      "Epoch 288/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2006 - accuracy: 0.8985\n",
      "Epoch 289/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1804 - accuracy: 0.9080\n",
      "Epoch 290/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2024 - accuracy: 0.8995\n",
      "Epoch 291/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1795 - accuracy: 0.9078\n",
      "Epoch 292/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1891 - accuracy: 0.9026\n",
      "Epoch 293/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1909 - accuracy: 0.9033\n",
      "Epoch 294/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1864 - accuracy: 0.9046\n",
      "Epoch 295/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1837 - accuracy: 0.9059\n",
      "Epoch 296/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1934 - accuracy: 0.9010\n",
      "Epoch 297/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1842 - accuracy: 0.9066\n",
      "Epoch 298/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1913 - accuracy: 0.9013\n",
      "Epoch 299/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1923 - accuracy: 0.9033\n",
      "Epoch 300/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1827 - accuracy: 0.9054\n",
      "Epoch 301/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1904 - accuracy: 0.9034\n",
      "Epoch 302/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1786 - accuracy: 0.9074\n",
      "Epoch 303/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1944 - accuracy: 0.9020\n",
      "Epoch 304/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1795 - accuracy: 0.9081\n",
      "Epoch 305/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1775 - accuracy: 0.9090\n",
      "Epoch 306/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1802 - accuracy: 0.9076\n",
      "Epoch 307/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1800 - accuracy: 0.9071\n",
      "Epoch 308/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1883 - accuracy: 0.9042\n",
      "Epoch 309/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1922 - accuracy: 0.9026\n",
      "Epoch 310/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1855 - accuracy: 0.9065\n",
      "Epoch 311/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1692 - accuracy: 0.9111\n",
      "Epoch 312/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1879 - accuracy: 0.9042\n",
      "Epoch 313/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1808 - accuracy: 0.9063\n",
      "Epoch 314/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1881 - accuracy: 0.9040\n",
      "Epoch 315/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1728 - accuracy: 0.9090\n",
      "Epoch 316/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1795 - accuracy: 0.9078\n",
      "Epoch 317/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1890 - accuracy: 0.9042\n",
      "Epoch 318/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1855 - accuracy: 0.9048\n",
      "Epoch 319/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1868 - accuracy: 0.9057\n",
      "Epoch 320/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1938 - accuracy: 0.9001\n",
      "Epoch 321/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1773 - accuracy: 0.9068\n",
      "Epoch 322/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1759 - accuracy: 0.9076\n",
      "Epoch 323/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1850 - accuracy: 0.9051\n",
      "Epoch 324/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1890 - accuracy: 0.9039\n",
      "Epoch 325/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1803 - accuracy: 0.9070\n",
      "Epoch 326/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1784 - accuracy: 0.9078\n",
      "Epoch 327/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1777 - accuracy: 0.9100\n",
      "Epoch 328/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1783 - accuracy: 0.9096\n",
      "Epoch 329/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1813 - accuracy: 0.9075\n",
      "Epoch 330/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1757 - accuracy: 0.9083\n",
      "Epoch 331/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1824 - accuracy: 0.9077\n",
      "Epoch 332/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1762 - accuracy: 0.9110\n",
      "Epoch 333/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1811 - accuracy: 0.9053\n",
      "Epoch 334/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1811 - accuracy: 0.9079\n",
      "Epoch 335/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1801 - accuracy: 0.9086\n",
      "Epoch 336/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1725 - accuracy: 0.9098\n",
      "Epoch 337/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1830 - accuracy: 0.9052\n",
      "Epoch 338/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1968 - accuracy: 0.9019\n",
      "Epoch 339/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1763 - accuracy: 0.9088\n",
      "Epoch 340/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1827 - accuracy: 0.9076\n",
      "Epoch 341/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1811 - accuracy: 0.9072\n",
      "Epoch 342/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1794 - accuracy: 0.9073\n",
      "Epoch 343/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1743 - accuracy: 0.9099\n",
      "Epoch 344/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1766 - accuracy: 0.9075\n",
      "Epoch 345/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1864 - accuracy: 0.9050\n",
      "Epoch 346/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1806 - accuracy: 0.9088\n",
      "Epoch 347/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1838 - accuracy: 0.9076\n",
      "Epoch 348/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1785 - accuracy: 0.9101\n",
      "Epoch 349/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1916 - accuracy: 0.9045\n",
      "Epoch 350/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1771 - accuracy: 0.9100\n",
      "Epoch 351/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1744 - accuracy: 0.9099\n",
      "Epoch 352/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.2061 - accuracy: 0.9014\n",
      "Epoch 353/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1790 - accuracy: 0.9085\n",
      "Epoch 354/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1757 - accuracy: 0.9095\n",
      "Epoch 355/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1681 - accuracy: 0.9144\n",
      "Epoch 356/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1671 - accuracy: 0.9128\n",
      "Epoch 357/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1885 - accuracy: 0.9047\n",
      "Epoch 358/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1864 - accuracy: 0.9065\n",
      "Epoch 359/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1670 - accuracy: 0.9128\n",
      "Epoch 360/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1797 - accuracy: 0.9074\n",
      "Epoch 361/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1792 - accuracy: 0.9083\n",
      "Epoch 362/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1865 - accuracy: 0.9046\n",
      "Epoch 363/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1770 - accuracy: 0.9088\n",
      "Epoch 364/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1692 - accuracy: 0.9135\n",
      "Epoch 365/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1701 - accuracy: 0.9121\n",
      "Epoch 366/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1818 - accuracy: 0.9065\n",
      "Epoch 367/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1711 - accuracy: 0.9120\n",
      "Epoch 368/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1824 - accuracy: 0.9078\n",
      "Epoch 369/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1866 - accuracy: 0.9047\n",
      "Epoch 370/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1771 - accuracy: 0.9108\n",
      "Epoch 371/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1757 - accuracy: 0.9086\n",
      "Epoch 372/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1763 - accuracy: 0.9099\n",
      "Epoch 373/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1835 - accuracy: 0.9078\n",
      "Epoch 374/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1765 - accuracy: 0.9113\n",
      "Epoch 375/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1685 - accuracy: 0.9131\n",
      "Epoch 376/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1774 - accuracy: 0.9114\n",
      "Epoch 377/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1742 - accuracy: 0.9109\n",
      "Epoch 378/600\n",
      "420/420 [==============================] - 4s 9ms/step - loss: 0.1851 - accuracy: 0.9063\n",
      "Epoch 379/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1760 - accuracy: 0.9103\n",
      "Epoch 380/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1749 - accuracy: 0.9105\n",
      "Epoch 381/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1639 - accuracy: 0.9142\n",
      "Epoch 382/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1818 - accuracy: 0.9095\n",
      "Epoch 383/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1713 - accuracy: 0.9124\n",
      "Epoch 384/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1792 - accuracy: 0.9100\n",
      "Epoch 385/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1847 - accuracy: 0.9082\n",
      "Epoch 386/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1622 - accuracy: 0.9155\n",
      "Epoch 387/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1751 - accuracy: 0.9094\n",
      "Epoch 388/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1679 - accuracy: 0.9132\n",
      "Epoch 389/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1751 - accuracy: 0.9103\n",
      "Epoch 390/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1662 - accuracy: 0.9143\n",
      "Epoch 391/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1675 - accuracy: 0.9129\n",
      "Epoch 392/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1763 - accuracy: 0.9098\n",
      "Epoch 393/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1757 - accuracy: 0.9109\n",
      "Epoch 394/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1835 - accuracy: 0.9081\n",
      "Epoch 395/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1723 - accuracy: 0.9107\n",
      "Epoch 396/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1664 - accuracy: 0.9125\n",
      "Epoch 397/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1719 - accuracy: 0.9119\n",
      "Epoch 398/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1835 - accuracy: 0.9075\n",
      "Epoch 399/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1805 - accuracy: 0.9102\n",
      "Epoch 400/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1753 - accuracy: 0.9089\n",
      "Epoch 401/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1819 - accuracy: 0.9094\n",
      "Epoch 402/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1845 - accuracy: 0.9072\n",
      "Epoch 403/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1725 - accuracy: 0.9126\n",
      "Epoch 404/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1596 - accuracy: 0.9179\n",
      "Epoch 405/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1790 - accuracy: 0.9085\n",
      "Epoch 406/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1584 - accuracy: 0.9177\n",
      "Epoch 407/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1728 - accuracy: 0.9130\n",
      "Epoch 408/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1786 - accuracy: 0.9099\n",
      "Epoch 409/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1758 - accuracy: 0.9107\n",
      "Epoch 410/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1688 - accuracy: 0.9131\n",
      "Epoch 411/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1703 - accuracy: 0.9117\n",
      "Epoch 412/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1661 - accuracy: 0.9135\n",
      "Epoch 413/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1865 - accuracy: 0.9077\n",
      "Epoch 414/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1662 - accuracy: 0.9145\n",
      "Epoch 415/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1661 - accuracy: 0.9143\n",
      "Epoch 416/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1819 - accuracy: 0.9085\n",
      "Epoch 417/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1573 - accuracy: 0.9177\n",
      "Epoch 418/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1639 - accuracy: 0.9148\n",
      "Epoch 419/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1847 - accuracy: 0.9080\n",
      "Epoch 420/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1769 - accuracy: 0.9110\n",
      "Epoch 421/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1575 - accuracy: 0.9187\n",
      "Epoch 422/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1702 - accuracy: 0.9137\n",
      "Epoch 423/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1885 - accuracy: 0.9062\n",
      "Epoch 424/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1634 - accuracy: 0.9155\n",
      "Epoch 425/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1648 - accuracy: 0.9152\n",
      "Epoch 426/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1661 - accuracy: 0.9130\n",
      "Epoch 427/600\n",
      "420/420 [==============================] - 4s 8ms/step - loss: 0.1640 - accuracy: 0.9168\n",
      "Epoch 428/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1771 - accuracy: 0.9108\n",
      "Epoch 429/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1610 - accuracy: 0.9151\n",
      "Epoch 430/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1609 - accuracy: 0.9158\n",
      "Epoch 431/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1689 - accuracy: 0.9131\n",
      "Epoch 432/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1846 - accuracy: 0.9070\n",
      "Epoch 433/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1618 - accuracy: 0.9156\n",
      "Epoch 434/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1673 - accuracy: 0.9137\n",
      "Epoch 435/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1696 - accuracy: 0.9133\n",
      "Epoch 436/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1634 - accuracy: 0.9148\n",
      "Epoch 437/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1751 - accuracy: 0.9118\n",
      "Epoch 438/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1582 - accuracy: 0.9164\n",
      "Epoch 439/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1872 - accuracy: 0.9070\n",
      "Epoch 440/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1665 - accuracy: 0.9150\n",
      "Epoch 441/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1717 - accuracy: 0.9128\n",
      "Epoch 442/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1699 - accuracy: 0.9126\n",
      "Epoch 443/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1545 - accuracy: 0.9198\n",
      "Epoch 444/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1633 - accuracy: 0.9152\n",
      "Epoch 445/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1675 - accuracy: 0.9145\n",
      "Epoch 446/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1703 - accuracy: 0.9125\n",
      "Epoch 447/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1641 - accuracy: 0.9162\n",
      "Epoch 448/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1631 - accuracy: 0.9160\n",
      "Epoch 449/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1613 - accuracy: 0.9156\n",
      "Epoch 450/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1697 - accuracy: 0.9119\n",
      "Epoch 451/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1601 - accuracy: 0.9161\n",
      "Epoch 452/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1707 - accuracy: 0.9129\n",
      "Epoch 453/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1691 - accuracy: 0.9160\n",
      "Epoch 454/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1706 - accuracy: 0.9125\n",
      "Epoch 455/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1718 - accuracy: 0.9148\n",
      "Epoch 456/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1620 - accuracy: 0.9161\n",
      "Epoch 457/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1568 - accuracy: 0.9185\n",
      "Epoch 458/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1754 - accuracy: 0.9125\n",
      "Epoch 459/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1651 - accuracy: 0.9149\n",
      "Epoch 460/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1659 - accuracy: 0.9132\n",
      "Epoch 461/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1569 - accuracy: 0.9178\n",
      "Epoch 462/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1855 - accuracy: 0.9093\n",
      "Epoch 463/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1571 - accuracy: 0.9172\n",
      "Epoch 464/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1662 - accuracy: 0.9134\n",
      "Epoch 465/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1655 - accuracy: 0.9148\n",
      "Epoch 466/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1774 - accuracy: 0.9102\n",
      "Epoch 467/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1579 - accuracy: 0.9163\n",
      "Epoch 468/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1855 - accuracy: 0.9067\n",
      "Epoch 469/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1671 - accuracy: 0.9132\n",
      "Epoch 470/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1528 - accuracy: 0.9190\n",
      "Epoch 471/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1667 - accuracy: 0.9135\n",
      "Epoch 472/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1644 - accuracy: 0.9154\n",
      "Epoch 473/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1809 - accuracy: 0.9100\n",
      "Epoch 474/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1615 - accuracy: 0.9160\n",
      "Epoch 475/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1616 - accuracy: 0.9161\n",
      "Epoch 476/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1539 - accuracy: 0.9195\n",
      "Epoch 477/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1668 - accuracy: 0.9164\n",
      "Epoch 478/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1737 - accuracy: 0.9119\n",
      "Epoch 479/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1631 - accuracy: 0.9150\n",
      "Epoch 480/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1590 - accuracy: 0.9180\n",
      "Epoch 481/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1707 - accuracy: 0.9144\n",
      "Epoch 482/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1636 - accuracy: 0.9169\n",
      "Epoch 483/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1760 - accuracy: 0.9132\n",
      "Epoch 484/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1556 - accuracy: 0.9176\n",
      "Epoch 485/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1613 - accuracy: 0.9172\n",
      "Epoch 486/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1588 - accuracy: 0.9184\n",
      "Epoch 487/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1682 - accuracy: 0.9144\n",
      "Epoch 488/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1690 - accuracy: 0.9146\n",
      "Epoch 489/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1631 - accuracy: 0.9148\n",
      "Epoch 490/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1642 - accuracy: 0.9144\n",
      "Epoch 491/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1637 - accuracy: 0.9160\n",
      "Epoch 492/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1748 - accuracy: 0.9118\n",
      "Epoch 493/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1643 - accuracy: 0.9149\n",
      "Epoch 494/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1648 - accuracy: 0.9150\n",
      "Epoch 495/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1562 - accuracy: 0.9189\n",
      "Epoch 496/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1618 - accuracy: 0.9178\n",
      "Epoch 497/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1497 - accuracy: 0.9205\n",
      "Epoch 498/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1553 - accuracy: 0.9183\n",
      "Epoch 499/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1803 - accuracy: 0.9094\n",
      "Epoch 500/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1721 - accuracy: 0.9131\n",
      "Epoch 501/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1673 - accuracy: 0.9160\n",
      "Epoch 502/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1647 - accuracy: 0.9154\n",
      "Epoch 503/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1573 - accuracy: 0.9176\n",
      "Epoch 504/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1740 - accuracy: 0.9133\n",
      "Epoch 505/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1606 - accuracy: 0.9180\n",
      "Epoch 506/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1568 - accuracy: 0.9200\n",
      "Epoch 507/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1633 - accuracy: 0.9171\n",
      "Epoch 508/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1630 - accuracy: 0.9146\n",
      "Epoch 509/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1609 - accuracy: 0.9166\n",
      "Epoch 510/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1618 - accuracy: 0.9179\n",
      "Epoch 511/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1710 - accuracy: 0.9143\n",
      "Epoch 512/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1544 - accuracy: 0.9187\n",
      "Epoch 513/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1664 - accuracy: 0.9144\n",
      "Epoch 514/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1642 - accuracy: 0.9142\n",
      "Epoch 515/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1570 - accuracy: 0.9181\n",
      "Epoch 516/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1571 - accuracy: 0.9174\n",
      "Epoch 517/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1664 - accuracy: 0.9157\n",
      "Epoch 518/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1608 - accuracy: 0.9179\n",
      "Epoch 519/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1691 - accuracy: 0.9147\n",
      "Epoch 520/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1752 - accuracy: 0.9118\n",
      "Epoch 521/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1606 - accuracy: 0.9170\n",
      "Epoch 522/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1591 - accuracy: 0.9177\n",
      "Epoch 523/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1778 - accuracy: 0.9105\n",
      "Epoch 524/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1719 - accuracy: 0.9133\n",
      "Epoch 525/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1788 - accuracy: 0.9117\n",
      "Epoch 526/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1577 - accuracy: 0.9175\n",
      "Epoch 527/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1489 - accuracy: 0.9223\n",
      "Epoch 528/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1575 - accuracy: 0.9181\n",
      "Epoch 529/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1570 - accuracy: 0.9195\n",
      "Epoch 530/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1784 - accuracy: 0.9122\n",
      "Epoch 531/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1613 - accuracy: 0.9168\n",
      "Epoch 532/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1542 - accuracy: 0.9205\n",
      "Epoch 533/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1736 - accuracy: 0.9133\n",
      "Epoch 534/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1719 - accuracy: 0.9133\n",
      "Epoch 535/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1532 - accuracy: 0.9196\n",
      "Epoch 536/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1760 - accuracy: 0.9116\n",
      "Epoch 537/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1594 - accuracy: 0.9182\n",
      "Epoch 538/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1599 - accuracy: 0.9178\n",
      "Epoch 539/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1556 - accuracy: 0.9185\n",
      "Epoch 540/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1484 - accuracy: 0.9217\n",
      "Epoch 541/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1621 - accuracy: 0.9168\n",
      "Epoch 542/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1656 - accuracy: 0.9156\n",
      "Epoch 543/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1786 - accuracy: 0.9104\n",
      "Epoch 544/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1636 - accuracy: 0.9156\n",
      "Epoch 545/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1657 - accuracy: 0.9160\n",
      "Epoch 546/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1634 - accuracy: 0.9159\n",
      "Epoch 547/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1563 - accuracy: 0.9196\n",
      "Epoch 548/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1711 - accuracy: 0.9133\n",
      "Epoch 549/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1600 - accuracy: 0.9170\n",
      "Epoch 550/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1527 - accuracy: 0.9195\n",
      "Epoch 551/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1494 - accuracy: 0.9211\n",
      "Epoch 552/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1655 - accuracy: 0.9147\n",
      "Epoch 553/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1580 - accuracy: 0.9187\n",
      "Epoch 554/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1770 - accuracy: 0.9116\n",
      "Epoch 555/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1606 - accuracy: 0.9173\n",
      "Epoch 556/600\n",
      "420/420 [==============================] - ETA: 0s - loss: 0.1532 - accuracy: 0.91 - 3s 8ms/step - loss: 0.1533 - accuracy: 0.9194\n",
      "Epoch 557/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1610 - accuracy: 0.9180\n",
      "Epoch 558/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1629 - accuracy: 0.9175\n",
      "Epoch 559/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1646 - accuracy: 0.9163\n",
      "Epoch 560/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1643 - accuracy: 0.9157\n",
      "Epoch 561/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1518 - accuracy: 0.9213\n",
      "Epoch 562/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1726 - accuracy: 0.9141\n",
      "Epoch 563/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1633 - accuracy: 0.9162\n",
      "Epoch 564/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1557 - accuracy: 0.9189\n",
      "Epoch 565/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1867 - accuracy: 0.9094\n",
      "Epoch 566/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1538 - accuracy: 0.9192\n",
      "Epoch 567/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1497 - accuracy: 0.9210\n",
      "Epoch 568/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1574 - accuracy: 0.9192\n",
      "Epoch 569/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1643 - accuracy: 0.9170\n",
      "Epoch 570/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1532 - accuracy: 0.9205\n",
      "Epoch 571/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1593 - accuracy: 0.9161\n",
      "Epoch 572/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1590 - accuracy: 0.9173\n",
      "Epoch 573/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1522 - accuracy: 0.9206\n",
      "Epoch 574/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1620 - accuracy: 0.9163\n",
      "Epoch 575/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1663 - accuracy: 0.9162\n",
      "Epoch 576/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1640 - accuracy: 0.9170\n",
      "Epoch 577/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1583 - accuracy: 0.9172\n",
      "Epoch 578/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1583 - accuracy: 0.9190\n",
      "Epoch 579/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1599 - accuracy: 0.9166\n",
      "Epoch 580/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1636 - accuracy: 0.9171\n",
      "Epoch 581/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1569 - accuracy: 0.9207\n",
      "Epoch 582/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1709 - accuracy: 0.9146\n",
      "Epoch 583/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1600 - accuracy: 0.9171\n",
      "Epoch 584/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1568 - accuracy: 0.9193\n",
      "Epoch 585/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1621 - accuracy: 0.9167\n",
      "Epoch 586/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1582 - accuracy: 0.9182\n",
      "Epoch 587/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1560 - accuracy: 0.9201\n",
      "Epoch 588/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1642 - accuracy: 0.9154\n",
      "Epoch 589/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1663 - accuracy: 0.9153\n",
      "Epoch 590/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1442 - accuracy: 0.9240\n",
      "Epoch 591/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1507 - accuracy: 0.9217\n",
      "Epoch 592/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1655 - accuracy: 0.9156\n",
      "Epoch 593/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1557 - accuracy: 0.9178\n",
      "Epoch 594/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1571 - accuracy: 0.9203\n",
      "Epoch 595/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1555 - accuracy: 0.9205\n",
      "Epoch 596/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1618 - accuracy: 0.9181\n",
      "Epoch 597/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1654 - accuracy: 0.9158\n",
      "Epoch 598/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1572 - accuracy: 0.9202\n",
      "Epoch 599/600\n",
      "420/420 [==============================] - 3s 7ms/step - loss: 0.1566 - accuracy: 0.9195\n",
      "Epoch 600/600\n",
      "420/420 [==============================] - 3s 8ms/step - loss: 0.1522 - accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(Train_Dataset,\n",
    "    Y_Train_Dataset,\n",
    "    batch_size=100,\n",
    "    epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 4ms/step - loss: 1.2490 - accuracy: 0.7971\n",
      "Accuracy: 79.71\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(Validation_Dataset, Y_Validation_Dataset)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Plot confusion matrix\n",
    "batch_size = 1024\n",
    "test_Y_hat = model.predict(Validation_Dataset, batch_size=1024)\n",
    "conf = np.zeros([len(mods),len(mods)])\n",
    "confnorm = np.zeros([len(mods),len(mods)])\n",
    "for i in range(0,Validation_Dataset.shape[0]):\n",
    "    j = list(Y_Validation_Dataset[i,:]).index(1)\n",
    "    k = int(np.argmax(test_Y_hat[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(mods)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "plot_confusion_matrix(confnorm, labels=mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8PSK 0.7895667550839964\n",
      "AM-DSB 0.6027049873203719\n",
      "BPSK 0.9835661462612982\n",
      "CPFSK 1.0\n",
      "GFSK 1.0\n",
      "PAM4 0.9644970414201184\n",
      "QAM16 0.4680511182108626\n",
      "QAM64 0.49355877616747185\n",
      "QPSK 0.8761583824768323\n",
      "WBFM 0.8140116763969975\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(confnorm)):\n",
    "    print(mods[i],confnorm[i,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emil",
   "language": "python",
   "name": "emil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
